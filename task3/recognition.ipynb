{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avashchilko/abbyy9sem/course_cvdl/.venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -e .\n",
    "#pip install  course_ocr_t3\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import pandas as pd\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = Path('/') / 'DATA' / 'asaginbaev' / 'CourseOCRTask3' \n",
    "\n",
    "TRAIN_PATH, TEST_PATH = DATASET_PATH / 'Train', DATASET_PATH / 'Test'\n",
    "\n",
    "assert DATASET_PATH.exists(), DATASET_PATH.absolute()\n",
    "assert TRAIN_PATH.exists(), TRAIN_PATH.absolute()\n",
    "assert TEST_PATH.exists(), TEST_PATH.absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8199 100\n",
      "torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n",
      "torch.Size([3, 512, 512]) 1105913212699e2e8a558191113acbd7.png\n",
      "7380 819\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import Resize, Compose, ToTensor\n",
    "import torch.nn as nn\n",
    "from task3pack.utils.data import SegmentationDataset\n",
    "\n",
    "IMAGE_SIZE = [512, 512]\n",
    "NAMES = ['path', 'code', 'x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'x4', 'y4', 'barcode']\n",
    "\n",
    "image_transforms = Compose([\n",
    "    ToTensor(),\n",
    "    Resize(IMAGE_SIZE),\n",
    "])\n",
    "\n",
    "target_transforms = Compose([\n",
    "    Resize(IMAGE_SIZE),   \n",
    "])\n",
    "\n",
    "train_dataset = SegmentationDataset(csv_path=TRAIN_PATH / 'markup.csv', images_path=TRAIN_PATH / 'Images', csv_header=NAMES, csv_encoding='utf-16', \n",
    "                                   image_transforms=image_transforms, target_transforms=target_transforms, shape=IMAGE_SIZE)\n",
    "test_dataset = SegmentationDataset(csv_path=None, images_path=TEST_PATH / 'Images', csv_header=NAMES, csv_encoding='utf-16', \n",
    "                                   image_transforms=image_transforms, target_transforms=target_transforms, shape=IMAGE_SIZE, is_test=True)\n",
    "\n",
    "print(len(train_dataset), len(test_dataset))\n",
    "print(train_dataset[0][0].shape, train_dataset[0][1].shape)\n",
    "print(test_dataset[0][0].shape, test_dataset[0][1])\n",
    "\n",
    "val_ratio = 0.1\n",
    "val_size = int(len(train_dataset) * val_ratio)\n",
    "train_size = len(train_dataset) - val_size\n",
    "\n",
    "print(train_size, val_size)\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset,\n",
    "        [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def four_point_transform(image, rect):\n",
    "    # obtain a consistent order of the points and unpack them\n",
    "    # individually\n",
    "    #rect = order_points(pts)\n",
    "    image = np.array(image)\n",
    "    tl, tr, br, bl = rect[0, :], rect[1, :], rect[2, :], rect[3, :]\n",
    "\n",
    "    # compute the width of the new image, which will be the\n",
    "    # maximum distance between bottom-right and bottom-left\n",
    "    # x-coordiates or the top-right and top-left x-coordinates\n",
    "    widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
    "    widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
    "    maxWidth = max(int(widthA), int(widthB))\n",
    "\n",
    "    # compute the height of the new image, which will be the\n",
    "    # maximum distance between the top-right and bottom-right\n",
    "    # y-coordinates or the top-left and bottom-left y-coordinates\n",
    "    heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
    "    heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
    "    maxHeight = max(int(heightA), int(heightB))\n",
    "\n",
    "    # now that we have the dimensions of the new image, construct\n",
    "    # the set of destination points to obtain a \"birds eye view\",\n",
    "    # (i.e. top-down view) of the image, again specifying points\n",
    "    # in the top-left, top-right, bottom-right, and bottom-left\n",
    "    # order\n",
    "    dst = np.array([\n",
    "        [0, 0],\n",
    "        [maxWidth - 1, 0],\n",
    "        [maxWidth - 1, maxHeight - 1],\n",
    "        [0, maxHeight - 1]], dtype = \"float32\")\n",
    "\n",
    "    # compute the perspective transform matrix and then apply it\n",
    "    M = cv2.getPerspectiveTransform(rect, dst)\n",
    "    warped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))\n",
    "\n",
    "    # return the warped image\n",
    "    return warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_hists(image, thresh_hist=99, thresh_image=50, width=1200):\n",
    "    image = cv2.resize(image, (width, 1000))\n",
    "    image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    th, image_th = cv2.threshold(image_gray, thresh_image, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    #images = []\n",
    "    hists = []\n",
    "    for i in range(1000 // 100):\n",
    "        img_crop = image_th[i * 100: (i + 1) * 100, :]\n",
    "        vertical_pixel_sum = np.sum(img_crop, axis=0)\n",
    "        myprojection = vertical_pixel_sum / 255\n",
    "        myprojection = np.where(myprojection > thresh_hist, 1, 0)\n",
    "        \n",
    "        #images.append(img_crop)\n",
    "        hists.append(myprojection[np.newaxis, ...])\n",
    "        \n",
    "    return np.concatenate(hists, axis=0)\n",
    "\n",
    "\n",
    "def dump_hists_to_npy(csv_path, images_path, csv_header, width=1200):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    markup = pd.read_csv(csv_path, header=None, encoding='utf-16', names=csv_header)\n",
    "    for idx in tqdm(range(len(markup))):\n",
    "        image_path = images_path / markup.loc[idx, 'path']\n",
    "        \n",
    "        if image_path.exists():\n",
    "            image = Image.open(image_path)\n",
    "            coords = markup.loc[idx, ['x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'x4', 'y4']].to_numpy(dtype='float32').reshape(4, 2)\n",
    "            warped_image = four_point_transform(image, coords)\n",
    "            hists = get_hists(warped_image, width)\n",
    "            resX = np.concatenate(hists[3:-3], axis=0)\n",
    "            \n",
    "            label = str(markup.loc[idx, 'code'])\n",
    "            \n",
    "            for _ in range(13 - len(label)):\n",
    "                label = '0' + label\n",
    "                \n",
    "            resy = []\n",
    "            for char in label:\n",
    "                resy.append(int(char))\n",
    "            \n",
    "            X.append(resX)\n",
    "            y.append(resy)\n",
    "        \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def dump_hist_to_npy(csv_path, images_path, csv_header, width=1200):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    markup = pd.read_csv(csv_path, header=None, encoding='utf-16', names=csv_header)\n",
    "    for idx in tqdm(range(len(markup))):\n",
    "        image_path = images_path / markup.loc[idx, 'path']\n",
    "        \n",
    "        if image_path.exists():\n",
    "            image = Image.open(image_path)\n",
    "            coords = markup.loc[idx, ['x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'x4', 'y4']].to_numpy(dtype='float32').reshape(4, 2)\n",
    "            warped_image = four_point_transform(image, coords)\n",
    "            hists = get_hists(warped_image, width=width)\n",
    "            resX = hists[5]\n",
    "            \n",
    "            label = str(markup.loc[idx, 'code'])\n",
    "            \n",
    "            for _ in range(13 - len(label)):\n",
    "                label = '0' + label\n",
    "                \n",
    "            resy = []\n",
    "            for char in label:\n",
    "                resy.append(int(char))\n",
    "            \n",
    "            X.append(resX)\n",
    "            y.append(resy)\n",
    "        \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work with reference bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## width = 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 8199/8199 [21:06<00:00,  6.47it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = dump_hist_to_npy(TRAIN_PATH / 'markup.csv', TRAIN_PATH / 'Images', NAMES)\n",
    "X_train.dump('X_train_ref_single.npy')\n",
    "y_train.dump('y_train_ref_single.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 11.28it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = dump_hist_to_npy('markup.csv', TEST_PATH / 'Images', NAMES)\n",
    "X_test.dump('X_test_ref_single.npy')\n",
    "y_test.dump('y_test_ref_single.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 8199/8199 [25:26<00:00,  5.37it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = dump_hists_to_npy(TRAIN_PATH / 'markup.csv', TRAIN_PATH / 'Images', NAMES)\n",
    "X_train.dump('X_train_ref.npy')\n",
    "y_train.dump('y_train_ref.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 100/100 [00:28<00:00,  3.53it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = dump_hists_to_npy('markup.csv', TEST_PATH / 'Images', NAMES)\n",
    "X_test.dump('X_test_ref.npy')\n",
    "y_test.dump('y_test_ref.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work with unet bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Width = 240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 8197/8197 [37:38<00:00,  3.63it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = dump_hist_to_npy('train_markup.csv', TRAIN_PATH / 'Images', NAMES, width=240)\n",
    "X_train.dump('X_train_ref_single_240.npy')\n",
    "y_train.dump('y_train_ref_single_240.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 100/100 [00:31<00:00,  3.17it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = dump_hist_to_npy('test_markup.csv', TEST_PATH / 'Images', NAMES, width=240)\n",
    "X_test.dump('X_test_ref_single_240.npy')\n",
    "y_test.dump('y_test_ref_single_240.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_scoreuracy_score\n",
    "import sklearn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = np.load('./X_train_ref.npy', allow_pickle=True), np.load('./y_train_ref.npy', allow_pickle=True)\n",
    "X_test, y_test = np.load('./X_test_ref.npy', allow_pickle=True), np.load('./y_test_ref.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              objective='multi:softprob', predictor=None, ...)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbclassifier = XGBClassifier()\n",
    "xgbclassifier.fit(X_train, y_train[:, 0])\n",
    "\n",
    "accuracy_score(xgbclassifier.predict(X_test), y_test[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.86\n",
      "1 0.86\n",
      "2 0.84\n",
      "3 0.78\n",
      "4 0.74\n",
      "5 0.66\n",
      "6 0.58\n"
     ]
    }
   ],
   "source": [
    "xgbs = []\n",
    "for i in range(y_train.shape[1]):\n",
    "    xgbc = XGBClassifier()\n",
    "    xgbc.fit(X_train, y_train[:, i])\n",
    "\n",
    "    print(i, accuracy_score(xgbc.predict(X_test), y_test[:, i]))\n",
    "    xgbs.append(xgbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        loss = model.loss(x, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    return train_losses\n",
    "\n",
    "\n",
    "def eval_loss(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            loss = model.loss(x, y)\n",
    "            total_loss += loss * x.shape[0]\n",
    "        avg_loss = total_loss / len(data_loader.dataset)\n",
    "    return avg_loss.item()\n",
    "\n",
    "\n",
    "def train_epochs(model, train_loader, test_loader, train_args, device):\n",
    "    epochs, lr = train_args['epochs'], train_args['lr']\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = [eval_loss(model, test_loader, device)]\n",
    "    for epoch in range(epochs):\n",
    "        print(f'epoch {epoch} started')\n",
    "        model.train()\n",
    "        train_losses.extend(train(model, train_loader, optimizer, device))\n",
    "        test_loss = eval_loss(model, test_loader, device)\n",
    "        test_losses.append(test_loss)\n",
    "        print('train loss: {}, test_loss: {}'.format(np.mean(train_losses[-1000:]), \n",
    "                                                     test_losses[-1]))\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "def train_model(train_dataset, test_dataset, model, train_dataloader_kwargs, test_dataloader_kwargs, training_kwargs, device='cpu'):\n",
    "    \"\"\"\n",
    "    train_data: A (n_train, H, W, 1) uint8 numpy array of binary images with values in {0, 1}\n",
    "    test_data: A (n_test, H, W, 1) uint8 numpy array of binary images with values in {0, 1}\n",
    "    model: nn.Model item, should contain function loss and accept\n",
    "    Returns\n",
    "    - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n",
    "    - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n",
    "    - trained model\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    test_losses = []\n",
    "    train_losses = []\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, **train_dataloader_kwargs)\n",
    "    test_dataloader = DataLoader(test_dataset, **test_dataloader_kwargs)\n",
    "\n",
    "    test_losses.append(eval_loss(model, test_dataloader, device))\n",
    "\n",
    "    train_loss, test_loss = train_epochs(model, train_dataloader, test_dataloader, training_kwargs, device)\n",
    "    test_losses += test_loss\n",
    "    train_losses += train_loss\n",
    "\n",
    "    return np.array(train_losses), np.array(test_losses), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader_config = {\n",
    "    'batch_size': 128,\n",
    "    'shuffle': True,\n",
    "}\n",
    "\n",
    "test_dataloader_config = {\n",
    "    'batch_size': 128,\n",
    "    'shuffle': False,\n",
    "}\n",
    "\n",
    "learning_config = {\n",
    "    'lr': 1e-3,\n",
    "    'epochs': 100,\n",
    "}\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=512, n_outputs=13, n_classes=10):\n",
    "        super().__init__()\n",
    "        self.n_outputs = n_outputs\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, n_outputs * n_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.model(x)\n",
    "        \n",
    "        #result = torch.zeros(x.shape[0], self.n_outputs)\n",
    "        #for i in range(self.n_outputs):\n",
    "        #    result[:, i] = torch.argmax(logits[:, i * self.n_classes: (i + 1) * self.n_classes], dim=-1)\n",
    "         \n",
    "        result = torch.argmax(logits, -1)    \n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        logits = self.model(x)\n",
    "        \n",
    "        #loss = torch.zeros(self.n_outputs)\n",
    "        #for i in range(self.n_outputs):\n",
    "        #    loss[i] = F.cross_entropy(logits[:, i * self.n_classes: (i + 1) * self.n_classes], y[:, i])\n",
    "            \n",
    "        \n",
    "        #return loss.sum()\n",
    "        return F.cross_entropy(logits, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Perceptron(240, n_outputs=1)\n",
    "\n",
    "train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train[:, 0]))\n",
    "test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.LongTensor(y_test[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0]) tensor(2)\n",
      "tensor([9]) tensor(8)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([2]) tensor(9)\n",
      "tensor([9]) tensor(9)\n",
      "tensor([9]) tensor(9)\n",
      "tensor([9]) tensor(9)\n",
      "tensor([9]) tensor(9)\n",
      "tensor([9]) tensor(9)\n",
      "tensor([9]) tensor(9)\n",
      "tensor([9]) tensor(9)\n",
      "tensor([9]) tensor(9)\n",
      "tensor([9]) tensor(9)\n",
      "tensor([9]) tensor(2)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([2]) tensor(9)\n",
      "tensor([9]) tensor(9)\n",
      "tensor([9]) tensor(9)\n",
      "tensor([9]) tensor(9)\n",
      "tensor([0]) tensor(4)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(2)\n",
      "tensor([5]) tensor(5)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([5]) tensor(4)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([2]) tensor(2)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([5]) tensor(8)\n",
      "tensor([9]) tensor(3)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(6)\n",
      "tensor([7]) tensor(8)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([0]) tensor(4)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([5]) tensor(4)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(9)\n",
      "tensor([2]) tensor(7)\n",
      "tensor([9]) tensor(6)\n",
      "tensor([0]) tensor(4)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([2]) tensor(4)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(2)\n",
      "tensor([9]) tensor(2)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(9)\n",
      "tensor([9]) tensor(5)\n",
      "tensor([9]) tensor(7)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(9)\n",
      "tensor([9]) tensor(9)\n",
      "tensor([9]) tensor(9)\n",
      "tensor([2]) tensor(3)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(3)\n",
      "tensor([9]) tensor(9)\n",
      "tensor([9]) tensor(9)\n",
      "tensor([9]) tensor(9)\n",
      "tensor([9]) tensor(5)\n",
      "tensor([9]) tensor(5)\n",
      "tensor([9]) tensor(5)\n",
      "tensor([9]) tensor(5)\n",
      "tensor([9]) tensor(5)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([7]) tensor(4)\n",
      "tensor([9]) tensor(5)\n",
      "tensor([9]) tensor(9)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(9)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([2]) tensor(4)\n"
     ]
    }
   ],
   "source": [
    "for x, y in test_dataset:\n",
    "    pred = model(x[None, ...])\n",
    "    print(pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 started\n",
      "train loss: 1.5004126796355615, test_loss: 1.4134961366653442\n",
      "epoch 1 started\n",
      "train loss: 1.4165156038907858, test_loss: 1.4789462089538574\n",
      "epoch 2 started\n",
      "train loss: 1.3742656075037443, test_loss: 1.3653132915496826\n",
      "epoch 3 started\n",
      "train loss: 1.3448397643291032, test_loss: 1.3308955430984497\n",
      "epoch 4 started\n",
      "train loss: 1.316778724193573, test_loss: 1.3372299671173096\n",
      "epoch 5 started\n",
      "train loss: 1.2931239291643486, test_loss: 1.2261842489242554\n",
      "epoch 6 started\n",
      "train loss: 1.2733773865542568, test_loss: 1.2476755380630493\n",
      "epoch 7 started\n",
      "train loss: 1.2551837312487455, test_loss: 1.3112170696258545\n",
      "epoch 8 started\n",
      "train loss: 1.240519638856252, test_loss: 1.185001015663147\n",
      "epoch 9 started\n",
      "train loss: 1.225677423110375, test_loss: 1.3537651300430298\n",
      "epoch 10 started\n",
      "train loss: 1.212048705974659, test_loss: 1.2798749208450317\n",
      "epoch 11 started\n",
      "train loss: 1.1997688489846694, test_loss: 1.1871891021728516\n",
      "epoch 12 started\n",
      "train loss: 1.1875505307016994, test_loss: 1.3692795038223267\n",
      "epoch 13 started\n",
      "train loss: 1.1755657086005578, test_loss: 1.3288933038711548\n",
      "epoch 14 started\n",
      "train loss: 1.1628695949071493, test_loss: 1.3317838907241821\n",
      "epoch 15 started\n",
      "train loss: 1.132916869416833, test_loss: 1.3540256023406982\n",
      "epoch 16 started\n",
      "train loss: 1.104483946070075, test_loss: 1.3643078804016113\n",
      "epoch 17 started\n",
      "train loss: 1.0812379788607358, test_loss: 1.3780109882354736\n",
      "epoch 18 started\n",
      "train loss: 1.057410490617156, test_loss: 1.3264180421829224\n",
      "epoch 19 started\n",
      "train loss: 1.0368376758545637, test_loss: 1.4654651880264282\n",
      "epoch 20 started\n",
      "train loss: 1.0177930881232022, test_loss: 1.3586134910583496\n",
      "epoch 21 started\n",
      "train loss: 0.9981401550322772, test_loss: 1.4155865907669067\n",
      "epoch 22 started\n",
      "train loss: 0.9797777902036905, test_loss: 1.5421323776245117\n",
      "epoch 23 started\n",
      "train loss: 0.9641622804552317, test_loss: 1.4918614625930786\n",
      "epoch 24 started\n",
      "train loss: 0.9474346980601549, test_loss: 1.4845473766326904\n",
      "epoch 25 started\n",
      "train loss: 0.931841063156724, test_loss: 1.5751053094863892\n",
      "epoch 26 started\n",
      "train loss: 0.9182966208606959, test_loss: 1.6199268102645874\n",
      "epoch 27 started\n",
      "train loss: 0.9022498971372843, test_loss: 1.614770770072937\n",
      "epoch 28 started\n",
      "train loss: 0.8867532873675227, test_loss: 1.6601866483688354\n",
      "epoch 29 started\n",
      "train loss: 0.8728945907279849, test_loss: 1.5894191265106201\n",
      "epoch 30 started\n",
      "train loss: 0.861125297434628, test_loss: 1.697306513786316\n",
      "epoch 31 started\n",
      "train loss: 0.8515184092596173, test_loss: 1.730713963508606\n",
      "epoch 32 started\n",
      "train loss: 0.8412574341073632, test_loss: 1.6362227201461792\n",
      "epoch 33 started\n",
      "train loss: 0.8307422206178308, test_loss: 1.666675090789795\n",
      "epoch 34 started\n",
      "train loss: 0.8200182711854577, test_loss: 1.6786601543426514\n",
      "epoch 35 started\n",
      "train loss: 0.8092983817830682, test_loss: 1.689672827720642\n",
      "epoch 36 started\n",
      "train loss: 0.8001051552668214, test_loss: 1.6554075479507446\n",
      "epoch 37 started\n",
      "train loss: 0.7919666921570897, test_loss: 1.843008279800415\n",
      "epoch 38 started\n",
      "train loss: 0.7817554647997021, test_loss: 1.8524906635284424\n",
      "epoch 39 started\n",
      "train loss: 0.772034628458321, test_loss: 1.955880880355835\n",
      "epoch 40 started\n",
      "train loss: 0.7627969051674008, test_loss: 1.9233506917953491\n",
      "epoch 41 started\n",
      "train loss: 0.7541972768977284, test_loss: 1.9106178283691406\n",
      "epoch 42 started\n",
      "train loss: 0.7475907952859998, test_loss: 1.7629237174987793\n",
      "epoch 43 started\n",
      "train loss: 0.7401686031594873, test_loss: 1.8900642395019531\n",
      "epoch 44 started\n",
      "train loss: 0.7358766609728337, test_loss: 2.016390323638916\n",
      "epoch 45 started\n",
      "train loss: 0.7281656403541565, test_loss: 2.0401408672332764\n",
      "epoch 46 started\n",
      "train loss: 0.7231438367068768, test_loss: 2.124438524246216\n",
      "epoch 47 started\n",
      "train loss: 0.7174973490834237, test_loss: 2.038475275039673\n",
      "epoch 48 started\n",
      "train loss: 0.7142015639543533, test_loss: 2.0239057540893555\n",
      "epoch 49 started\n",
      "train loss: 0.7113482576012612, test_loss: 2.097583293914795\n",
      "epoch 50 started\n",
      "train loss: 0.7076837252676487, test_loss: 2.163893461227417\n",
      "epoch 51 started\n",
      "train loss: 0.702355919599533, test_loss: 2.1634628772735596\n",
      "epoch 52 started\n",
      "train loss: 0.6987706962525845, test_loss: 2.081315040588379\n",
      "epoch 53 started\n",
      "train loss: 0.695585606366396, test_loss: 2.102902889251709\n",
      "epoch 54 started\n",
      "train loss: 0.6924976779520512, test_loss: 2.346949577331543\n",
      "epoch 55 started\n",
      "train loss: 0.689735434293747, test_loss: 2.2150914669036865\n",
      "epoch 56 started\n",
      "train loss: 0.6873880990445614, test_loss: 2.176654100418091\n",
      "epoch 57 started\n",
      "train loss: 0.6875649652779102, test_loss: 2.17903208732605\n",
      "epoch 58 started\n",
      "train loss: 0.6857726409435272, test_loss: 2.3214006423950195\n",
      "epoch 59 started\n",
      "train loss: 0.6837109076678753, test_loss: 2.2016234397888184\n",
      "epoch 60 started\n",
      "train loss: 0.6813028473854065, test_loss: 2.240736484527588\n",
      "epoch 61 started\n",
      "train loss: 0.6796171791255474, test_loss: 2.433917760848999\n",
      "epoch 62 started\n",
      "train loss: 0.6798403008878231, test_loss: 2.215024471282959\n",
      "epoch 63 started\n",
      "train loss: 0.6768527850806713, test_loss: 2.3034746646881104\n",
      "epoch 64 started\n",
      "train loss: 0.6721746437251568, test_loss: 2.3199191093444824\n",
      "epoch 65 started\n",
      "train loss: 0.6709648597836494, test_loss: 2.429682493209839\n",
      "epoch 66 started\n",
      "train loss: 0.6694945961534977, test_loss: 2.368638277053833\n",
      "epoch 67 started\n",
      "train loss: 0.6671584195494652, test_loss: 2.2693674564361572\n",
      "epoch 68 started\n",
      "train loss: 0.6658922348618508, test_loss: 2.409466505050659\n",
      "epoch 69 started\n",
      "train loss: 0.6657260306477547, test_loss: 2.692617893218994\n",
      "epoch 70 started\n",
      "train loss: 0.663340814486146, test_loss: 2.438620090484619\n",
      "epoch 71 started\n",
      "train loss: 0.6611976453214884, test_loss: 2.587890386581421\n",
      "epoch 72 started\n",
      "train loss: 0.6567701909989119, test_loss: 2.554746627807617\n",
      "epoch 73 started\n",
      "train loss: 0.6542789628058672, test_loss: 2.581568956375122\n",
      "epoch 74 started\n",
      "train loss: 0.6506237247809767, test_loss: 2.480607509613037\n",
      "epoch 75 started\n",
      "train loss: 0.6487247305437922, test_loss: 2.605997323989868\n",
      "epoch 76 started\n",
      "train loss: 0.6464756156727671, test_loss: 2.6732852458953857\n",
      "epoch 77 started\n",
      "train loss: 0.6445337890610099, test_loss: 2.803637981414795\n",
      "epoch 78 started\n",
      "train loss: 0.6411976843699813, test_loss: 2.7976138591766357\n",
      "epoch 79 started\n",
      "train loss: 0.6404813471660018, test_loss: 2.724086284637451\n",
      "epoch 80 started\n",
      "train loss: 0.638919569246471, test_loss: 2.7606770992279053\n",
      "epoch 81 started\n",
      "train loss: 0.6367564481720328, test_loss: 2.9156763553619385\n",
      "epoch 82 started\n",
      "train loss: 0.6353225189968944, test_loss: 2.679030179977417\n",
      "epoch 83 started\n",
      "train loss: 0.6346814263686538, test_loss: 2.755556344985962\n",
      "epoch 84 started\n",
      "train loss: 0.633135708488524, test_loss: 2.8045623302459717\n",
      "epoch 85 started\n",
      "train loss: 0.6306367065683007, test_loss: 2.8651082515716553\n",
      "epoch 86 started\n",
      "train loss: 0.6298631975874305, test_loss: 2.799408197402954\n",
      "epoch 87 started\n",
      "train loss: 0.6279983098432421, test_loss: 2.9204301834106445\n",
      "epoch 88 started\n",
      "train loss: 0.6263945854540943, test_loss: 2.699753522872925\n",
      "epoch 89 started\n",
      "train loss: 0.6250316443081975, test_loss: 2.8341405391693115\n",
      "epoch 90 started\n",
      "train loss: 0.6264851547639846, test_loss: 2.6738393306732178\n",
      "epoch 91 started\n",
      "train loss: 0.6265775465231418, test_loss: 2.8380777835845947\n",
      "epoch 92 started\n",
      "train loss: 0.625942294852066, test_loss: 2.669766902923584\n",
      "epoch 93 started\n",
      "train loss: 0.6265863348107814, test_loss: 2.881192684173584\n",
      "epoch 94 started\n",
      "train loss: 0.6266160171788215, test_loss: 2.769540786743164\n",
      "epoch 95 started\n",
      "train loss: 0.626562490062046, test_loss: 2.6155261993408203\n",
      "epoch 96 started\n",
      "train loss: 0.6270790282350063, test_loss: 2.763991117477417\n",
      "epoch 97 started\n",
      "train loss: 0.6274643180828571, test_loss: 2.9264023303985596\n",
      "epoch 98 started\n",
      "train loss: 0.6268570952873707, test_loss: 2.737253427505493\n",
      "epoch 99 started\n",
      "train loss: 0.6261950418215274, test_loss: 2.9780306816101074\n"
     ]
    }
   ],
   "source": [
    "train_losses, test_losses, trained_model = train_model(train_dataset, test_dataset, model, train_dataloader_config, test_dataloader_config, learning_config, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4]) tensor(2)\n",
      "tensor([4]) tensor(8)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(9)\n",
      "tensor([9]) tensor(9)\n",
      "tensor([4]) tensor(9)\n",
      "tensor([9]) tensor(9)\n",
      "tensor([9]) tensor(9)\n",
      "tensor([9]) tensor(9)\n",
      "tensor([9]) tensor(9)\n",
      "tensor([9]) tensor(9)\n",
      "tensor([8]) tensor(9)\n",
      "tensor([8]) tensor(9)\n",
      "tensor([4]) tensor(2)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([9]) tensor(9)\n",
      "tensor([2]) tensor(9)\n",
      "tensor([5]) tensor(9)\n",
      "tensor([9]) tensor(9)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([2]) tensor(2)\n",
      "tensor([4]) tensor(5)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([9]) tensor(2)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(8)\n",
      "tensor([3]) tensor(3)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(6)\n",
      "tensor([4]) tensor(8)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([5]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(9)\n",
      "tensor([4]) tensor(7)\n",
      "tensor([6]) tensor(6)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([8]) tensor(2)\n",
      "tensor([2]) tensor(2)\n",
      "tensor([7]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([5]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(9)\n",
      "tensor([5]) tensor(5)\n",
      "tensor([4]) tensor(7)\n",
      "tensor([9]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(9)\n",
      "tensor([4]) tensor(9)\n",
      "tensor([4]) tensor(9)\n",
      "tensor([9]) tensor(3)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(3)\n",
      "tensor([4]) tensor(9)\n",
      "tensor([4]) tensor(9)\n",
      "tensor([4]) tensor(9)\n",
      "tensor([4]) tensor(5)\n",
      "tensor([4]) tensor(5)\n",
      "tensor([4]) tensor(5)\n",
      "tensor([4]) tensor(5)\n",
      "tensor([4]) tensor(5)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(5)\n",
      "tensor([9]) tensor(9)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([5]) tensor(4)\n",
      "tensor([4]) tensor(9)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(4)\n",
      "tensor([4]) tensor(4)\n"
     ]
    }
   ],
   "source": [
    "for x, y in test_dataset:\n",
    "    pred = trained_model(x[None, ...])\n",
    "    print(pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
