{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avashchilko/abbyy9sem/course_cvdl/.venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -e .\n",
    "#pip install  course_ocr_t3\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import pandas as pd\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = Path('/') / 'DATA' / 'asaginbaev' / 'CourseOCRTask3' \n",
    "\n",
    "TRAIN_PATH, TEST_PATH = DATASET_PATH / 'Train', DATASET_PATH / 'Test'\n",
    "\n",
    "assert DATASET_PATH.exists(), DATASET_PATH.absolute()\n",
    "assert TRAIN_PATH.exists(), TRAIN_PATH.absolute()\n",
    "assert TEST_PATH.exists(), TEST_PATH.absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8199 100\n",
      "torch.Size([3, 512, 512]) torch.Size([1, 512, 512])\n",
      "torch.Size([3, 512, 512]) 1105913212699e2e8a558191113acbd7.png\n",
      "7380 819\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import Resize, Compose, ToTensor\n",
    "import torch.nn as nn\n",
    "from task3pack.utils.data import SegmentationDataset\n",
    "\n",
    "IMAGE_SIZE = [512, 512]\n",
    "NAMES = ['path', 'code', 'x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'x4', 'y4', 'barcode']\n",
    "\n",
    "image_transforms = Compose([\n",
    "    ToTensor(),\n",
    "    Resize(IMAGE_SIZE),\n",
    "])\n",
    "\n",
    "target_transforms = Compose([\n",
    "    Resize(IMAGE_SIZE),   \n",
    "])\n",
    "\n",
    "train_dataset = SegmentationDataset(csv_path=TRAIN_PATH / 'markup.csv', images_path=TRAIN_PATH / 'Images', csv_header=NAMES, csv_encoding='utf-16', \n",
    "                                   image_transforms=image_transforms, target_transforms=target_transforms, shape=IMAGE_SIZE)\n",
    "test_dataset = SegmentationDataset(csv_path=None, images_path=TEST_PATH / 'Images', csv_header=NAMES, csv_encoding='utf-16', \n",
    "                                   image_transforms=image_transforms, target_transforms=target_transforms, shape=IMAGE_SIZE, is_test=True)\n",
    "\n",
    "print(len(train_dataset), len(test_dataset))\n",
    "print(train_dataset[0][0].shape, train_dataset[0][1].shape)\n",
    "print(test_dataset[0][0].shape, test_dataset[0][1])\n",
    "\n",
    "val_ratio = 0.1\n",
    "val_size = int(len(train_dataset) * val_ratio)\n",
    "train_size = len(train_dataset) - val_size\n",
    "\n",
    "print(train_size, val_size)\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset,\n",
    "        [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def four_point_transform(image, rect):\n",
    "    # obtain a consistent order of the points and unpack them\n",
    "    # individually\n",
    "    #rect = order_points(pts)\n",
    "    image = np.array(image)\n",
    "    tl, tr, br, bl = rect[0, :], rect[1, :], rect[2, :], rect[3, :]\n",
    "\n",
    "    # compute the width of the new image, which will be the\n",
    "    # maximum distance between bottom-right and bottom-left\n",
    "    # x-coordiates or the top-right and top-left x-coordinates\n",
    "    widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
    "    widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
    "    maxWidth = max(int(widthA), int(widthB))\n",
    "\n",
    "    # compute the height of the new image, which will be the\n",
    "    # maximum distance between the top-right and bottom-right\n",
    "    # y-coordinates or the top-left and bottom-left y-coordinates\n",
    "    heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
    "    heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
    "    maxHeight = max(int(heightA), int(heightB))\n",
    "\n",
    "    # now that we have the dimensions of the new image, construct\n",
    "    # the set of destination points to obtain a \"birds eye view\",\n",
    "    # (i.e. top-down view) of the image, again specifying points\n",
    "    # in the top-left, top-right, bottom-right, and bottom-left\n",
    "    # order\n",
    "    dst = np.array([\n",
    "        [0, 0],\n",
    "        [maxWidth - 1, 0],\n",
    "        [maxWidth - 1, maxHeight - 1],\n",
    "        [0, maxHeight - 1]], dtype = \"float32\")\n",
    "\n",
    "    # compute the perspective transform matrix and then apply it\n",
    "    M = cv2.getPerspectiveTransform(rect, dst)\n",
    "    warped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))\n",
    "\n",
    "    # return the warped image\n",
    "    return warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_hists(image, thresh_hist=99, thresh_image=50, width=1200):\n",
    "    image = cv2.resize(image, (width, 1000))\n",
    "    image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    th, image_th = cv2.threshold(image_gray, thresh_image, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    #images = []\n",
    "    hists = []\n",
    "    for i in range(1000 // 100):\n",
    "        img_crop = image_th[i * 100: (i + 1) * 100, :]\n",
    "        vertical_pixel_sum = np.sum(img_crop, axis=0)\n",
    "        myprojection = vertical_pixel_sum / 255\n",
    "        myprojection = np.where(myprojection > thresh_hist, 1, 0)\n",
    "        \n",
    "        #images.append(img_crop)\n",
    "        hists.append(myprojection[np.newaxis, ...])\n",
    "        \n",
    "    return np.concatenate(hists, axis=0)\n",
    "\n",
    "\n",
    "def dump_hists_to_npy(csv_path, images_path, csv_header, width=1200):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    markup = pd.read_csv(csv_path, header=None, encoding='utf-16', names=csv_header)\n",
    "    for idx in tqdm(range(len(markup))):\n",
    "        image_path = images_path / markup.loc[idx, 'path']\n",
    "        \n",
    "        if image_path.exists():\n",
    "            image = Image.open(image_path)\n",
    "            coords = markup.loc[idx, ['x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'x4', 'y4']].to_numpy(dtype='float32').reshape(4, 2)\n",
    "            warped_image = four_point_transform(image, coords)\n",
    "            hists = get_hists(warped_image, width)\n",
    "            resX = np.concatenate(hists[3:-3], axis=0)\n",
    "            \n",
    "            label = str(markup.loc[idx, 'code'])\n",
    "            \n",
    "            for _ in range(13 - len(label)):\n",
    "                label = '0' + label\n",
    "                \n",
    "            resy = []\n",
    "            for char in label:\n",
    "                resy.append(int(char))\n",
    "            \n",
    "            X.append(resX)\n",
    "            y.append(resy)\n",
    "        \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def dump_hist_to_npy(csv_path, images_path, csv_header, width=1200):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    markup = pd.read_csv(csv_path, header=None, encoding='utf-16', names=csv_header)\n",
    "    for idx in tqdm(range(len(markup))):\n",
    "        image_path = images_path / markup.loc[idx, 'path']\n",
    "        \n",
    "        if image_path.exists():\n",
    "            image = Image.open(image_path)\n",
    "            coords = markup.loc[idx, ['x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'x4', 'y4']].to_numpy(dtype='float32').reshape(4, 2)\n",
    "            warped_image = four_point_transform(image, coords)\n",
    "            hists = get_hists(warped_image, width=width)\n",
    "            resX = hists[5]\n",
    "            \n",
    "            label = str(markup.loc[idx, 'code'])\n",
    "            \n",
    "            for _ in range(13 - len(label)):\n",
    "                label = '0' + label\n",
    "                \n",
    "            resy = []\n",
    "            for char in label:\n",
    "                resy.append(int(char))\n",
    "            \n",
    "            X.append(resX)\n",
    "            y.append(resy)\n",
    "        \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work with reference bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## width = 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 8199/8199 [21:06<00:00,  6.47it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = dump_hist_to_npy(TRAIN_PATH / 'markup.csv', TRAIN_PATH / 'Images', NAMES)\n",
    "X_train.dump('X_train_ref_single.npy')\n",
    "y_train.dump('y_train_ref_single.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 11.28it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = dump_hist_to_npy('markup.csv', TEST_PATH / 'Images', NAMES)\n",
    "X_test.dump('X_test_ref_single.npy')\n",
    "y_test.dump('y_test_ref_single.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 8199/8199 [25:26<00:00,  5.37it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = dump_hists_to_npy(TRAIN_PATH / 'markup.csv', TRAIN_PATH / 'Images', NAMES)\n",
    "X_train.dump('X_train_ref.npy')\n",
    "y_train.dump('y_train_ref.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 100/100 [00:28<00:00,  3.53it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = dump_hists_to_npy('markup.csv', TEST_PATH / 'Images', NAMES)\n",
    "X_test.dump('X_test_ref.npy')\n",
    "y_test.dump('y_test_ref.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work with unet bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Width = 240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|██████████▎                                                                    | 1065/8197 [08:42<16:37,  7.15it/s]"
     ]
    }
   ],
   "source": [
    "X_train, y_train = dump_hist_to_npy('train_markup.csv', TRAIN_PATH / 'Images', NAMES, width=240)\n",
    "X_train.dump('X_train_ref_single_240.npy')\n",
    "y_train.dump('y_train_ref_single_240.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = dump_hist_to_npy('test_markup.csv', TEST_PATH / 'Images', NAMES, width=240)\n",
    "X_test.dump('X_test_ref_single_240.npy')\n",
    "y_test.dump('y_test_ref_single_240.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_scoreuracy_score\n",
    "import sklearn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = np.load('./X_train_ref_single.npy', allow_pickle=True), np.load('./y_train_ref_single.npy', allow_pickle=True)\n",
    "X_test, y_test = np.load('./X_test_ref_single.npy', allow_pickle=True), np.load('./y_test_ref_single.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              objective='multi:softprob', predictor=None, ...)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbclassifier = XGBClassifier()\n",
    "xgbclassifier.fit(X_train, y_train[:, 0])\n",
    "\n",
    "accuracy_score(xgbclassifier.predict(X_test), y_test[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.86\n",
      "1 0.86\n",
      "2 0.84\n",
      "3 0.78\n",
      "4 0.74\n",
      "5 0.66\n",
      "6 0.58\n"
     ]
    }
   ],
   "source": [
    "xgbs = []\n",
    "for i in range(y_train.shape[1]):\n",
    "    xgbc = XGBClassifier()\n",
    "    xgbc.fit(X_train, y_train[:, i])\n",
    "\n",
    "    print(i, accuracy_score(xgbc.predict(X_test), y_test[:, i]))\n",
    "    xgbs.append(xgbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        loss = model.loss(x, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    return train_losses\n",
    "\n",
    "\n",
    "def eval_loss(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            loss = model.loss(x, y)\n",
    "            total_loss += loss * x.shape[0]\n",
    "        avg_loss = total_loss / len(data_loader.dataset)\n",
    "    return avg_loss.item()\n",
    "\n",
    "\n",
    "def train_epochs(model, train_loader, test_loader, train_args, device):\n",
    "    epochs, lr = train_args['epochs'], train_args['lr']\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = [eval_loss(model, test_loader, device)]\n",
    "    for epoch in range(epochs):\n",
    "        print(f'epoch {epoch} started')\n",
    "        model.train()\n",
    "        train_losses.extend(train(model, train_loader, optimizer, device))\n",
    "        test_loss = eval_loss(model, test_loader, device)\n",
    "        test_losses.append(test_loss)\n",
    "        print('train loss: {}, test_loss: {}'.format(np.mean(train_losses[-1000:]), \n",
    "                                                     test_losses[-1]))\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "def train_model(train_dataset, test_dataset, model, train_dataloader_kwargs, test_dataloader_kwargs, training_kwargs, device='cpu'):\n",
    "    \"\"\"\n",
    "    train_data: A (n_train, H, W, 1) uint8 numpy array of binary images with values in {0, 1}\n",
    "    test_data: A (n_test, H, W, 1) uint8 numpy array of binary images with values in {0, 1}\n",
    "    model: nn.Model item, should contain function loss and accept\n",
    "    Returns\n",
    "    - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n",
    "    - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n",
    "    - trained model\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    test_losses = []\n",
    "    train_losses = []\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, **train_dataloader_kwargs)\n",
    "    test_dataloader = DataLoader(test_dataset, **test_dataloader_kwargs)\n",
    "\n",
    "    test_losses.append(eval_loss(model, test_dataloader, device))\n",
    "\n",
    "    train_loss, test_loss = train_epochs(model, train_dataloader, test_dataloader, training_kwargs, device)\n",
    "    test_losses += test_loss\n",
    "    train_losses += train_loss\n",
    "\n",
    "    return np.array(train_losses), np.array(test_losses), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader_config = {\n",
    "    'batch_size': 128,\n",
    "    'shuffle': True,\n",
    "}\n",
    "\n",
    "test_dataloader_config = {\n",
    "    'batch_size': 128,\n",
    "    'shuffle': False,\n",
    "}\n",
    "\n",
    "learning_config = {\n",
    "    'lr': 1e-3,\n",
    "    'epochs': 100,\n",
    "}\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=512, n_outputs=13, n_classes=10):\n",
    "        super().__init__()\n",
    "        self.n_outputs = n_outputs\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, n_outputs * n_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.model(x)\n",
    "        \n",
    "        result = torch.zeros(x.shape[0], self.n_outputs)\n",
    "        for i in range(self.n_outputs):\n",
    "            result[:, i] = torch.argmax(logits[:, i * self.n_classes: (i + 1) * self.n_classes], dim=-1)\n",
    "         \n",
    "        #result = torch.argmax(logits, -1)    \n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        logits = self.model(x)\n",
    "        \n",
    "        loss = torch.zeros(self.n_outputs)\n",
    "        for i in range(self.n_outputs):\n",
    "            loss[i] = F.cross_entropy(logits[:, i * self.n_classes: (i + 1) * self.n_classes], y[:, i])\n",
    "            \n",
    "        \n",
    "        return loss.sum()\n",
    "        #return F.cross_entropy(logits, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Perceptron(1200, n_outputs=13)\n",
    "\n",
    "train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))\n",
    "test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.LongTensor(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 1., 8., 1., 7., 3., 5., 5., 6., 8., 5., 0., 8.]]) tensor([2, 0, 0, 5, 0, 0, 3, 7, 0, 8, 8, 1, 4])\n",
      "tensor([[8., 1., 8., 4., 9., 3., 0., 9., 1., 8., 8., 7., 0.]]) tensor([8, 5, 9, 4, 0, 1, 5, 3, 0, 1, 0, 1, 3])\n",
      "tensor([[4., 1., 8., 2., 5., 1., 5., 5., 9., 2., 5., 0., 0.]]) tensor([4, 6, 0, 7, 0, 1, 6, 2, 4, 5, 6, 7, 6])\n",
      "tensor([[3., 1., 7., 2., 5., 0., 9., 5., 1., 3., 2., 0., 8.]]) tensor([4, 6, 0, 2, 8, 2, 4, 0, 1, 9, 4, 2, 6])\n",
      "tensor([[2., 1., 7., 6., 8., 1., 0., 9., 2., 9., 4., 6., 0.]]) tensor([9, 7, 8, 5, 9, 4, 7, 2, 3, 1, 2, 5, 0])\n",
      "tensor([[2., 0., 8., 4., 5., 1., 9., 5., 6., 3., 2., 0., 8.]]) tensor([9, 7, 8, 5, 4, 6, 9, 0, 0, 5, 7, 6, 6])\n",
      "tensor([[2., 0., 8., 9., 9., 6., 4., 6., 2., 9., 9., 0., 8.]]) tensor([9, 7, 8, 5, 9, 8, 1, 2, 4, 1, 1, 2, 3])\n",
      "tensor([[0., 0., 8., 0., 0., 5., 8., 9., 2., 3., 5., 8., 0.]]) tensor([9, 7, 8, 5, 8, 2, 9, 1, 1, 6, 6, 2, 0])\n",
      "tensor([[4., 1., 8., 6., 0., 3., 3., 5., 6., 9., 5., 6., 5.]]) tensor([9, 7, 8, 5, 3, 5, 4, 0, 0, 9, 8, 0, 0])\n",
      "tensor([[8., 1., 6., 1., 0., 7., 4., 9., 2., 3., 5., 2., 8.]]) tensor([9, 7, 8, 5, 9, 5, 7, 9, 0, 0, 9, 4, 8])\n",
      "tensor([[0., 1., 8., 3., 5., 1., 3., 5., 8., 7., 5., 6., 8.]]) tensor([9, 7, 8, 5, 9, 4, 7, 7, 4, 2, 2, 3, 7])\n",
      "tensor([[2., 1., 6., 9., 0., 5., 0., 5., 6., 9., 5., 6., 7.]]) tensor([9, 7, 8, 5, 9, 4, 1, 5, 7, 5, 6, 2, 6])\n",
      "tensor([[0., 3., 4., 2., 5., 1., 3., 4., 5., 9., 5., 6., 0.]]) tensor([9, 7, 8, 0, 1, 9, 4, 7, 0, 2, 2, 5, 6])\n",
      "tensor([[2., 0., 8., 0., 5., 3., 6., 3., 9., 9., 7., 8., 8.]]) tensor([9, 7, 8, 0, 1, 9, 4, 3, 5, 7, 3, 4, 0])\n",
      "tensor([[2., 1., 7., 0., 8., 5., 8., 4., 2., 8., 5., 8., 0.]]) tensor([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 2])\n",
      "tensor([[3., 0., 7., 2., 5., 0., 8., 7., 9., 0., 5., 6., 0.]]) tensor([4, 6, 0, 1, 4, 5, 0, 0, 3, 0, 0, 1, 0])\n",
      "tensor([[2., 1., 8., 4., 0., 4., 6., 5., 9., 8., 8., 6., 8.]]) tensor([4, 6, 1, 0, 0, 0, 8, 5, 2, 1, 6, 7, 2])\n",
      "tensor([[4., 1., 7., 0., 9., 3., 9., 5., 6., 1., 5., 6., 7.]]) tensor([9, 7, 8, 3, 5, 9, 6, 5, 1, 2, 7, 9, 9])\n",
      "tensor([[6., 1., 2., 0., 9., 3., 0., 9., 5., 9., 5., 6., 0.]]) tensor([9, 7, 8, 5, 9, 5, 4, 2, 0, 0, 6, 5, 2])\n",
      "tensor([[0., 1., 7., 4., 9., 3., 5., 5., 1., 8., 5., 6., 0.]]) tensor([9, 7, 8, 5, 6, 9, 9, 5, 3, 1, 8, 9, 9])\n",
      "tensor([[4., 1., 2., 0., 9., 1., 4., 4., 6., 8., 5., 6., 0.]]) tensor([9, 7, 8, 5, 8, 1, 3, 8, 0, 7, 8, 2, 4])\n",
      "tensor([[0., 1., 4., 9., 5., 0., 6., 9., 6., 7., 7., 0., 0.]]) tensor([4, 6, 0, 0, 6, 0, 5, 0, 1, 7, 3, 3, 3])\n",
      "tensor([[2., 1., 8., 9., 9., 5., 4., 6., 6., 1., 7., 6., 8.]]) tensor([4, 6, 8, 0, 0, 0, 1, 8, 2, 0, 0, 1, 0])\n",
      "tensor([[0., 1., 7., 9., 8., 3., 5., 5., 6., 9., 9., 6., 0.]]) tensor([2, 1, 0, 0, 1, 0, 0, 0, 3, 4, 5, 5, 9])\n",
      "tensor([[2., 1., 8., 0., 9., 6., 5., 5., 2., 2., 8., 6., 8.]]) tensor([5, 5, 5, 0, 1, 1, 8, 9, 0, 1, 2, 3, 2])\n",
      "tensor([[8., 2., 8., 1., 9., 3., 0., 5., 9., 9., 2., 6., 1.]]) tensor([4, 6, 0, 7, 1, 2, 9, 6, 3, 5, 3, 5, 7])\n",
      "tensor([[4., 1., 7., 0., 5., 5., 8., 9., 2., 2., 9., 6., 3.]]) tensor([4, 6, 0, 7, 9, 3, 1, 8, 8, 3, 2, 6, 7])\n",
      "tensor([[3., 1., 8., 6., 5., 1., 8., 5., 1., 9., 5., 9., 0.]]) tensor([4, 6, 0, 3, 3, 2, 2, 5, 6, 7, 7, 7, 8])\n",
      "tensor([[6., 0., 6., 0., 0., 0., 6., 9., 6., 3., 5., 6., 0.]]) tensor([4, 6, 0, 3, 2, 2, 3, 0, 6, 9, 7, 7, 7])\n",
      "tensor([[3., 0., 7., 2., 8., 0., 6., 5., 9., 3., 5., 6., 5.]]) tensor([4, 6, 0, 3, 5, 3, 1, 1, 0, 3, 6, 6, 8])\n",
      "tensor([[2., 1., 8., 6., 3., 1., 4., 6., 9., 9., 5., 4., 8.]]) tensor([4, 6, 0, 2, 7, 3, 1, 7, 4, 1, 1, 2, 7])\n",
      "tensor([[4., 1., 8., 6., 9., 1., 9., 5., 6., 9., 5., 6., 0.]]) tensor([2, 4, 0, 0, 0, 0, 0, 0, 0, 6, 4, 1, 1])\n",
      "tensor([[0., 9., 9., 6., 8., 0., 4., 5., 6., 3., 5., 6., 0.]]) tensor([4, 8, 1, 0, 6, 1, 3, 0, 0, 0, 4, 1, 2])\n",
      "tensor([[8., 0., 5., 2., 0., 3., 4., 6., 0., 1., 2., 6., 7.]]) tensor([8, 6, 1, 3, 0, 1, 1, 0, 0, 1, 0, 1, 4])\n",
      "tensor([[2., 1., 8., 1., 3., 0., 8., 9., 6., 9., 5., 8., 6.]]) tensor([3, 8, 0, 0, 0, 1, 0, 6, 4, 3, 7, 6, 4])\n",
      "tensor([[8., 1., 5., 2., 9., 0., 3., 9., 5., 1., 2., 5., 8.]]) tensor([4, 6, 5, 0, 0, 6, 7, 7, 6, 0, 1, 3, 1])\n",
      "tensor([[2., 1., 8., 0., 8., 1., 3., 5., 5., 9., 9., 6., 6.]]) tensor([4, 6, 0, 7, 0, 0, 7, 3, 6, 1, 3, 8, 5])\n",
      "tensor([[0., 0., 8., 0., 5., 0., 3., 5., 9., 7., 7., 6., 8.]]) tensor([6, 9, 3, 9, 5, 4, 0, 5, 8, 8, 8, 8, 1])\n",
      "tensor([[4., 1., 7., 1., 5., 1., 3., 9., 9., 2., 5., 0., 8.]]) tensor([8, 0, 0, 9, 7, 4, 0, 8, 7, 9, 7, 3, 2])\n",
      "tensor([[0., 1., 7., 6., 9., 4., 3., 5., 2., 2., 4., 4., 8.]]) tensor([4, 6, 0, 0, 8, 5, 1, 0, 1, 1, 5, 5, 0])\n",
      "tensor([[3., 1., 8., 8., 9., 0., 9., 5., 2., 1., 7., 4., 8.]]) tensor([4, 8, 1, 2, 3, 0, 2, 0, 0, 0, 0, 5, 5])\n",
      "tensor([[3., 1., 7., 2., 5., 4., 6., 5., 6., 2., 5., 6., 8.]]) tensor([4, 6, 0, 7, 1, 1, 4, 9, 1, 1, 9, 9, 2])\n",
      "tensor([[3., 1., 8., 0., 8., 1., 3., 5., 1., 9., 2., 7., 0.]]) tensor([4, 6, 0, 7, 1, 8, 3, 8, 1, 9, 7, 6, 2])\n",
      "tensor([[4., 9., 8., 0., 8., 1., 9., 8., 9., 9., 6., 6., 0.]]) tensor([4, 6, 3, 0, 0, 1, 4, 8, 5, 1, 5, 8, 7])\n",
      "tensor([[8., 1., 8., 2., 8., 1., 9., 9., 2., 3., 5., 0., 8.]]) tensor([4, 6, 0, 7, 1, 1, 9, 7, 0, 1, 8, 3, 3])\n",
      "tensor([[2., 0., 8., 4., 9., 3., 9., 5., 9., 0., 9., 8., 7.]]) tensor([4, 6, 0, 7, 1, 1, 9, 7, 0, 0, 4, 4, 7])\n",
      "tensor([[4., 1., 8., 6., 5., 3., 9., 4., 1., 0., 5., 8., 0.]]) tensor([4, 6, 8, 0, 0, 2, 1, 8, 8, 2, 2, 9, 6])\n",
      "tensor([[0., 3., 4., 0., 9., 3., 3., 7., 6., 9., 7., 6., 0.]]) tensor([9, 7, 8, 5, 9, 0, 6, 8, 3, 7, 4, 2, 4])\n",
      "tensor([[7., 3., 7., 9., 9., 3., 4., 4., 2., 2., 6., 8., 0.]]) tensor([7, 8, 9, 0, 0, 0, 0, 0, 0, 8, 3, 9, 6])\n",
      "tensor([[0., 0., 8., 4., 5., 1., 3., 5., 6., 7., 9., 6., 8.]]) tensor([6, 9, 4, 1, 0, 5, 7, 4, 0, 4, 8, 5, 1])\n",
      "tensor([[0., 1., 7., 2., 9., 1., 3., 5., 8., 9., 5., 6., 0.]]) tensor([4, 6, 1, 0, 0, 2, 8, 8, 9, 0, 1, 3, 0])\n",
      "tensor([[4., 3., 7., 3., 5., 0., 4., 9., 9., 3., 8., 6., 7.]]) tensor([4, 6, 0, 1, 2, 4, 8, 0, 2, 0, 1, 9, 3])\n",
      "tensor([[8., 1., 8., 2., 7., 0., 9., 9., 2., 3., 2., 4., 0.]]) tensor([4, 6, 0, 1, 2, 4, 8, 0, 1, 6, 3, 1, 8])\n",
      "tensor([[6., 1., 7., 1., 5., 5., 4., 9., 6., 8., 2., 7., 7.]]) tensor([4, 6, 0, 1, 2, 4, 8, 0, 1, 5, 5, 3, 3])\n",
      "tensor([[6., 1., 8., 6., 5., 1., 8., 9., 8., 0., 5., 0., 0.]]) tensor([4, 6, 0, 7, 0, 4, 1, 5, 6, 0, 0, 2, 7])\n",
      "tensor([[2., 1., 6., 1., 0., 0., 3., 5., 2., 3., 2., 8., 0.]]) tensor([4, 6, 0, 1, 2, 4, 8, 0, 1, 6, 0, 0, 4])\n",
      "tensor([[4., 9., 7., 6., 9., 3., 3., 5., 1., 9., 9., 6., 7.]]) tensor([4, 6, 0, 7, 0, 4, 3, 2, 0, 0, 9, 6, 9])\n",
      "tensor([[2., 1., 8., 1., 3., 4., 3., 5., 6., 1., 5., 6., 8.]]) tensor([4, 6, 7, 0, 0, 0, 1, 8, 4, 0, 6, 5, 1])\n",
      "tensor([[0., 1., 6., 2., 9., 0., 3., 5., 1., 9., 1., 8., 0.]]) tensor([4, 6, 0, 0, 9, 1, 9, 8, 1, 3, 1, 3, 3])\n",
      "tensor([[0., 0., 6., 3., 9., 1., 9., 5., 2., 3., 4., 4., 7.]]) tensor([2, 0, 4, 0, 0, 0, 0, 4, 4, 4, 4, 4, 0])\n",
      "tensor([[2., 3., 7., 9., 5., 0., 3., 5., 6., 3., 4., 8., 0.]]) tensor([2, 0, 4, 0, 0, 0, 0, 4, 4, 9, 4, 3, 8])\n",
      "tensor([[0., 1., 5., 1., 5., 0., 3., 5., 2., 2., 7., 0., 8.]]) tensor([4, 6, 5, 1, 1, 1, 1, 6, 1, 1, 1, 3, 3])\n",
      "tensor([[2., 2., 8., 0., 9., 4., 8., 5., 6., 0., 5., 6., 7.]]) tensor([4, 6, 0, 6, 0, 0, 8, 3, 5, 3, 8, 9, 4])\n",
      "tensor([[2., 1., 8., 2., 5., 0., 8., 4., 2., 7., 7., 8., 3.]]) tensor([4, 6, 0, 6, 1, 8, 0, 0, 0, 3, 6, 8, 6])\n",
      "tensor([[4., 1., 8., 1., 9., 0., 9., 9., 6., 8., 6., 4., 8.]]) tensor([4, 6, 0, 7, 0, 5, 0, 4, 6, 0, 8, 1, 3])\n",
      "tensor([[3., 1., 8., 2., 8., 1., 8., 7., 1., 1., 6., 6., 5.]]) tensor([4, 6, 0, 0, 9, 9, 9, 0, 2, 4, 5, 7, 3])\n",
      "tensor([[8., 1., 8., 0., 0., 1., 9., 5., 6., 3., 5., 3., 0.]]) tensor([4, 6, 0, 7, 0, 3, 5, 8, 9, 0, 5, 3, 6])\n",
      "tensor([[8., 2., 4., 1., 9., 0., 6., 5., 8., 0., 5., 6., 8.]]) tensor([4, 6, 2, 7, 0, 8, 9, 7, 4, 0, 6, 1, 8])\n",
      "tensor([[2., 2., 8., 2., 8., 0., 2., 6., 2., 9., 5., 8., 0.]]) tensor([4, 6, 0, 0, 6, 0, 5, 0, 1, 7, 6, 5, 4])\n",
      "tensor([[3., 1., 8., 2., 3., 0., 8., 9., 6., 8., 5., 3., 8.]]) tensor([9, 7, 8, 5, 9, 6, 9, 3, 0, 3, 0, 8, 9])\n",
      "tensor([[2., 1., 7., 9., 0., 1., 5., 5., 1., 1., 7., 6., 0.]]) tensor([5, 9, 0, 0, 5, 1, 6, 3, 2, 0, 3, 2, 4])\n",
      "tensor([[4., 1., 8., 4., 9., 3., 8., 5., 1., 8., 6., 6., 0.]]) tensor([7, 6, 2, 2, 1, 0, 0, 9, 2, 6, 8, 7, 0])\n",
      "tensor([[6., 1., 7., 9., 3., 0., 3., 4., 5., 3., 8., 6., 7.]]) tensor([4, 6, 0, 7, 0, 3, 1, 2, 4, 0, 6, 9, 4])\n",
      "tensor([[8., 1., 8., 9., 9., 6., 3., 4., 9., 9., 8., 0., 0.]]) tensor([4, 6, 8, 0, 0, 3, 6, 9, 1, 0, 7, 8, 6])\n",
      "tensor([[2., 0., 2., 2., 5., 0., 8., 5., 6., 3., 7., 6., 8.]]) tensor([9, 7, 8, 5, 3, 9, 6, 0, 0, 3, 4, 9, 1])\n",
      "tensor([[2., 1., 1., 9., 5., 1., 4., 5., 5., 9., 7., 0., 0.]]) tensor([9, 7, 8, 5, 3, 9, 6, 0, 0, 4, 5, 7, 3])\n",
      "tensor([[0., 1., 7., 6., 5., 3., 4., 9., 6., 2., 9., 6., 0.]]) tensor([9, 7, 8, 5, 8, 1, 2, 5, 1, 8, 4, 3, 1])\n",
      "tensor([[0., 1., 7., 1., 5., 1., 8., 9., 2., 8., 7., 6., 5.]]) tensor([3, 5, 7, 4, 6, 6, 1, 3, 6, 5, 2, 3, 7])\n",
      "tensor([[8., 0., 8., 1., 8., 6., 9., 9., 8., 6., 6., 6., 8.]]) tensor([4, 6, 0, 0, 9, 4, 9, 1, 2, 3, 1, 3, 4])\n",
      "tensor([[6., 1., 1., 2., 9., 0., 3., 9., 1., 9., 2., 8., 6.]]) tensor([4, 6, 2, 0, 0, 0, 4, 9, 5, 3, 5, 7, 5])\n",
      "tensor([[2., 0., 8., 7., 0., 0., 9., 5., 6., 1., 5., 6., 7.]]) tensor([3, 6, 0, 0, 5, 2, 2, 3, 6, 5, 6, 4, 8])\n",
      "tensor([[8., 0., 6., 6., 9., 4., 4., 5., 1., 8., 9., 6., 8.]]) tensor([9, 7, 8, 5, 9, 3, 0, 1, 0, 0, 2, 6, 6])\n",
      "tensor([[2., 1., 8., 9., 9., 4., 0., 5., 9., 9., 2., 6., 8.]]) tensor([9, 7, 8, 5, 1, 7, 0, 9, 1, 4, 2, 2, 7])\n",
      "tensor([[8., 1., 5., 5., 0., 0., 4., 5., 9., 9., 5., 6., 6.]]) tensor([9, 7, 8, 5, 9, 1, 7, 5, 9, 5, 3, 6, 8])\n",
      "tensor([[4., 1., 7., 2., 9., 6., 9., 9., 6., 8., 5., 0., 0.]]) tensor([5, 0, 5, 4, 1, 3, 1, 0, 5, 0, 3, 6, 1])\n",
      "tensor([[2., 2., 8., 2., 9., 3., 3., 5., 5., 2., 5., 6., 7.]]) tensor([5, 0, 5, 4, 1, 3, 1, 0, 5, 2, 1, 1, 2])\n",
      "tensor([[6., 1., 7., 9., 5., 0., 3., 9., 2., 0., 4., 6., 1.]]) tensor([5, 0, 5, 4, 1, 3, 1, 0, 4, 0, 1, 8, 8])\n",
      "tensor([[4., 1., 8., 3., 9., 1., 3., 5., 9., 9., 2., 6., 8.]]) tensor([5, 0, 1, 0, 9, 9, 3, 3, 8, 8, 0, 8, 0])\n",
      "tensor([[3., 1., 7., 2., 5., 5., 3., 5., 5., 8., 9., 8., 8.]]) tensor([5, 0, 1, 0, 9, 9, 3, 4, 5, 5, 2, 9, 4])\n",
      "tensor([[3., 0., 8., 3., 8., 0., 4., 5., 9., 3., 5., 0., 0.]]) tensor([4, 8, 9, 7, 0, 5, 6, 8, 4, 3, 6, 8, 4])\n",
      "tensor([[8., 1., 9., 0., 9., 0., 4., 5., 8., 0., 3., 6., 7.]]) tensor([4, 6, 0, 2, 7, 0, 1, 2, 0, 1, 4, 8, 7])\n",
      "tensor([[8., 0., 8., 9., 5., 4., 9., 4., 9., 9., 5., 0., 0.]]) tensor([4, 6, 0, 7, 1, 7, 1, 7, 8, 0, 3, 7, 1])\n",
      "tensor([[8., 1., 4., 1., 5., 0., 8., 5., 2., 9., 5., 6., 5.]]) tensor([5, 9, 9, 5, 3, 2, 7, 2, 7, 5, 1, 4, 7])\n",
      "tensor([[3., 3., 8., 5., 5., 1., 6., 5., 6., 9., 6., 3., 0.]]) tensor([9, 7, 8, 5, 1, 7, 0, 7, 3, 6, 3, 6, 2])\n",
      "tensor([[2., 1., 6., 6., 7., 3., 0., 5., 1., 9., 7., 6., 0.]]) tensor([4, 6, 0, 1, 7, 1, 3, 0, 0, 1, 7, 0, 2])\n",
      "tensor([[8., 7., 7., 2., 9., 4., 4., 9., 5., 9., 5., 6., 8.]]) tensor([4, 6, 0, 6, 6, 3, 1, 3, 4, 0, 0, 0, 1])\n",
      "tensor([[2., 3., 8., 2., 8., 0., 9., 4., 5., 9., 7., 8., 0.]]) tensor([9, 0, 0, 1, 4, 1, 4, 2, 0, 4, 0, 4, 7])\n",
      "tensor([[0., 1., 6., 5., 9., 7., 6., 5., 2., 0., 4., 6., 0.]]) tensor([4, 8, 1, 0, 0, 2, 3, 0, 0, 0, 1, 5, 6])\n",
      "tensor([[0., 1., 8., 3., 9., 1., 3., 5., 6., 9., 7., 6., 0.]]) tensor([4, 6, 1, 0, 0, 1, 2, 0, 4, 4, 8, 7, 7])\n",
      "tensor([[6., 0., 8., 0., 8., 3., 4., 5., 5., 3., 8., 6., 0.]]) tensor([4, 7, 4, 0, 0, 1, 8, 1, 3, 5, 3, 0, 6])\n"
     ]
    }
   ],
   "source": [
    "for x, y in test_dataset:\n",
    "    pred = model(x[None, ...])\n",
    "    print(pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 started\n",
      "train loss: 26.309514940701998, test_loss: 25.054397583007812\n",
      "epoch 1 started\n",
      "train loss: 25.173946688725398, test_loss: 22.10198974609375\n",
      "epoch 2 started\n",
      "train loss: 24.156404289832484, test_loss: 20.42795753479004\n",
      "epoch 3 started\n",
      "train loss: 23.32364813731267, test_loss: 19.10289192199707\n",
      "epoch 4 started\n",
      "train loss: 22.609312943678635, test_loss: 18.229127883911133\n",
      "epoch 5 started\n",
      "train loss: 22.037624941116725, test_loss: 17.2947998046875\n",
      "epoch 6 started\n",
      "train loss: 21.552847357110664, test_loss: 17.003389358520508\n",
      "epoch 7 started\n",
      "train loss: 21.14854193834158, test_loss: 16.397985458374023\n",
      "epoch 8 started\n",
      "train loss: 20.768507000523755, test_loss: 16.134353637695312\n",
      "epoch 9 started\n",
      "train loss: 20.436002380664533, test_loss: 16.06511116027832\n",
      "epoch 10 started\n",
      "train loss: 20.132732872862917, test_loss: 15.642287254333496\n",
      "epoch 11 started\n",
      "train loss: 19.858689283713318, test_loss: 15.548881530761719\n",
      "epoch 12 started\n",
      "train loss: 19.59904399962115, test_loss: 15.428667068481445\n",
      "epoch 13 started\n",
      "train loss: 19.359795407934502, test_loss: 15.32000732421875\n",
      "epoch 14 started\n",
      "train loss: 19.1387123244848, test_loss: 15.537281036376953\n",
      "epoch 15 started\n",
      "train loss: 18.618993275642396, test_loss: 15.277859687805176\n",
      "epoch 16 started\n",
      "train loss: 18.025228676795958, test_loss: 14.797685623168945\n",
      "epoch 17 started\n",
      "train loss: 17.54696537399292, test_loss: 15.34301471710205\n",
      "epoch 18 started\n",
      "train loss: 17.171079053878785, test_loss: 15.045125961303711\n",
      "epoch 19 started\n",
      "train loss: 16.84536482810974, test_loss: 14.949872016906738\n",
      "epoch 20 started\n",
      "train loss: 16.578661885261536, test_loss: 14.845043182373047\n",
      "epoch 21 started\n",
      "train loss: 16.32250426864624, test_loss: 14.871559143066406\n",
      "epoch 22 started\n",
      "train loss: 16.081408034324646, test_loss: 14.994882583618164\n",
      "epoch 23 started\n",
      "train loss: 15.86828811264038, test_loss: 14.793858528137207\n",
      "epoch 24 started\n",
      "train loss: 15.663809542655946, test_loss: 14.969705581665039\n",
      "epoch 25 started\n",
      "train loss: 15.484620692253113, test_loss: 14.803603172302246\n",
      "epoch 26 started\n",
      "train loss: 15.312849486351013, test_loss: 14.665685653686523\n",
      "epoch 27 started\n",
      "train loss: 15.1398907995224, test_loss: 14.96164321899414\n",
      "epoch 28 started\n",
      "train loss: 15.001966203689575, test_loss: 14.782479286193848\n",
      "epoch 29 started\n",
      "train loss: 14.873108246803284, test_loss: 14.8799409866333\n",
      "epoch 30 started\n",
      "train loss: 14.729100563049316, test_loss: 15.116290092468262\n",
      "epoch 31 started\n",
      "train loss: 14.596457711219788, test_loss: 14.695962905883789\n",
      "epoch 32 started\n",
      "train loss: 14.46878537940979, test_loss: 14.708560943603516\n",
      "epoch 33 started\n",
      "train loss: 14.353728231430054, test_loss: 14.891225814819336\n",
      "epoch 34 started\n",
      "train loss: 14.237694929122926, test_loss: 14.712638854980469\n",
      "epoch 35 started\n",
      "train loss: 14.127483986854553, test_loss: 14.896228790283203\n",
      "epoch 36 started\n",
      "train loss: 14.032516370773315, test_loss: 14.591899871826172\n",
      "epoch 37 started\n",
      "train loss: 13.932834586143494, test_loss: 15.02832317352295\n",
      "epoch 38 started\n",
      "train loss: 13.853415231704712, test_loss: 14.781519889831543\n",
      "epoch 39 started\n",
      "train loss: 13.75198321533203, test_loss: 14.746639251708984\n",
      "epoch 40 started\n",
      "train loss: 13.658447359085082, test_loss: 15.032746315002441\n",
      "epoch 41 started\n",
      "train loss: 13.575270246505736, test_loss: 14.917550086975098\n",
      "epoch 42 started\n",
      "train loss: 13.507686118125916, test_loss: 15.04017448425293\n",
      "epoch 43 started\n",
      "train loss: 13.42140482711792, test_loss: 15.062047958374023\n",
      "epoch 44 started\n",
      "train loss: 13.333948004722595, test_loss: 15.114801406860352\n",
      "epoch 45 started\n",
      "train loss: 13.249914973735809, test_loss: 14.9952974319458\n",
      "epoch 46 started\n",
      "train loss: 13.182407069683075, test_loss: 15.120253562927246\n",
      "epoch 47 started\n",
      "train loss: 13.100234995365144, test_loss: 14.9663667678833\n",
      "epoch 48 started\n",
      "train loss: 13.033075341701508, test_loss: 15.052783012390137\n",
      "epoch 49 started\n",
      "train loss: 12.968776826381683, test_loss: 15.500904083251953\n",
      "epoch 50 started\n",
      "train loss: 12.904940040111542, test_loss: 15.28858757019043\n",
      "epoch 51 started\n",
      "train loss: 12.834486382007599, test_loss: 15.095372200012207\n",
      "epoch 52 started\n",
      "train loss: 12.769941212177276, test_loss: 15.07144546508789\n",
      "epoch 53 started\n",
      "train loss: 12.691724057674408, test_loss: 15.255785942077637\n",
      "epoch 54 started\n",
      "train loss: 12.631053639411926, test_loss: 14.930732727050781\n",
      "epoch 55 started\n",
      "train loss: 12.575221723556519, test_loss: 15.17908000946045\n",
      "epoch 56 started\n",
      "train loss: 12.506332387924195, test_loss: 14.937617301940918\n",
      "epoch 57 started\n",
      "train loss: 12.447139820098878, test_loss: 15.15999984741211\n",
      "epoch 58 started\n",
      "train loss: 12.382062260627746, test_loss: 15.323031425476074\n",
      "epoch 59 started\n",
      "train loss: 12.337318044662476, test_loss: 15.183404922485352\n",
      "epoch 60 started\n",
      "train loss: 12.278886690139771, test_loss: 15.281323432922363\n",
      "epoch 61 started\n",
      "train loss: 12.226286674499512, test_loss: 15.308974266052246\n",
      "epoch 62 started\n",
      "train loss: 12.180741738319396, test_loss: 15.05059814453125\n",
      "epoch 63 started\n",
      "train loss: 12.123592707157135, test_loss: 15.252872467041016\n",
      "epoch 64 started\n",
      "train loss: 12.076879189968109, test_loss: 15.29166030883789\n",
      "epoch 65 started\n",
      "train loss: 12.0121455950737, test_loss: 15.13531494140625\n",
      "epoch 66 started\n",
      "train loss: 11.967617552280426, test_loss: 15.260601043701172\n",
      "epoch 67 started\n",
      "train loss: 11.948239648342133, test_loss: 15.548541069030762\n",
      "epoch 68 started\n",
      "train loss: 11.90045616197586, test_loss: 15.17409896850586\n",
      "epoch 69 started\n",
      "train loss: 11.84484003829956, test_loss: 15.46601676940918\n",
      "epoch 70 started\n",
      "train loss: 11.801856285572052, test_loss: 15.482555389404297\n",
      "epoch 71 started\n",
      "train loss: 11.760753791809082, test_loss: 15.471765518188477\n",
      "epoch 72 started\n",
      "train loss: 11.732218364715576, test_loss: 15.591297149658203\n",
      "epoch 73 started\n",
      "train loss: 11.70349522447586, test_loss: 15.364112854003906\n",
      "epoch 74 started\n",
      "train loss: 11.672167336940765, test_loss: 15.416357040405273\n",
      "epoch 75 started\n",
      "train loss: 11.634939220905304, test_loss: 15.7504301071167\n",
      "epoch 76 started\n",
      "train loss: 11.603451764583587, test_loss: 15.767306327819824\n",
      "epoch 77 started\n",
      "train loss: 11.57927433681488, test_loss: 15.577840805053711\n",
      "epoch 78 started\n",
      "train loss: 11.541165229797363, test_loss: 15.173436164855957\n",
      "epoch 79 started\n",
      "train loss: 11.525079495429992, test_loss: 15.790213584899902\n",
      "epoch 80 started\n",
      "train loss: 11.506848585128784, test_loss: 15.696131706237793\n",
      "epoch 81 started\n",
      "train loss: 11.476094630241395, test_loss: 15.94245719909668\n",
      "epoch 82 started\n",
      "train loss: 11.446552253723144, test_loss: 15.356130599975586\n",
      "epoch 83 started\n",
      "train loss: 11.393820580482483, test_loss: 15.712597846984863\n",
      "epoch 84 started\n",
      "train loss: 11.392389529228211, test_loss: 15.78877067565918\n",
      "epoch 85 started\n",
      "train loss: 11.377507615566254, test_loss: 15.549905776977539\n",
      "epoch 86 started\n",
      "train loss: 11.366479565143585, test_loss: 15.746698379516602\n",
      "epoch 87 started\n",
      "train loss: 11.331602329254151, test_loss: 15.642419815063477\n",
      "epoch 88 started\n",
      "train loss: 11.298516299247742, test_loss: 15.685503005981445\n",
      "epoch 89 started\n",
      "train loss: 11.268263317584992, test_loss: 15.75599479675293\n",
      "epoch 90 started\n",
      "train loss: 11.23298479127884, test_loss: 15.731531143188477\n",
      "epoch 91 started\n",
      "train loss: 11.200953056812287, test_loss: 15.542157173156738\n",
      "epoch 92 started\n",
      "train loss: 11.193232429027557, test_loss: 15.911532402038574\n",
      "epoch 93 started\n",
      "train loss: 11.173874142169952, test_loss: 15.851531028747559\n",
      "epoch 94 started\n",
      "train loss: 11.144937479496003, test_loss: 15.87745475769043\n",
      "epoch 95 started\n",
      "train loss: 11.109215353012084, test_loss: 16.05731964111328\n",
      "epoch 96 started\n",
      "train loss: 11.095169405937195, test_loss: 15.701162338256836\n",
      "epoch 97 started\n",
      "train loss: 11.05249918270111, test_loss: 16.05912971496582\n",
      "epoch 98 started\n",
      "train loss: 11.045005306959153, test_loss: 15.885849952697754\n",
      "epoch 99 started\n",
      "train loss: 10.98934792304039, test_loss: 15.954813003540039\n"
     ]
    }
   ],
   "source": [
    "train_losses, test_losses, trained_model = train_model(train_dataset, test_dataset, model, train_dataloader_config, test_dataloader_config, learning_config, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 9., 0., 1., 0., 2., 6., 7., 0., 6., 6., 1., 6.]]) tensor([2, 0, 0, 5, 0, 0, 3, 7, 0, 8, 8, 1, 4])\n",
      "tensor([[8., 2., 0., 0., 0., 2., 5., 3., 0., 1., 0., 2., 7.]]) tensor([8, 5, 9, 4, 0, 1, 5, 3, 0, 1, 0, 1, 3])\n",
      "tensor([[4., 6., 0., 7., 0., 1., 8., 2., 4., 5., 6., 3., 7.]]) tensor([4, 6, 0, 7, 0, 1, 6, 2, 4, 5, 6, 7, 6])\n",
      "tensor([[4., 6., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]]) tensor([4, 6, 0, 2, 8, 2, 4, 0, 1, 9, 4, 2, 6])\n",
      "tensor([[9., 7., 8., 5., 9., 4., 7., 2., 3., 1., 2., 4., 0.]]) tensor([9, 7, 8, 5, 9, 4, 7, 2, 3, 1, 2, 5, 0])\n",
      "tensor([[9., 7., 8., 5., 1., 8., 8., 3., 0., 5., 3., 5., 8.]]) tensor([9, 7, 8, 5, 4, 6, 9, 0, 0, 5, 7, 6, 6])\n",
      "tensor([[9., 7., 8., 5., 9., 2., 1., 3., 4., 6., 3., 6., 6.]]) tensor([9, 7, 8, 5, 9, 8, 1, 2, 4, 1, 1, 2, 3])\n",
      "tensor([[9., 7., 8., 5., 9., 0., 0., 4., 8., 6., 6., 1., 6.]]) tensor([9, 7, 8, 5, 8, 2, 9, 1, 1, 6, 6, 2, 0])\n",
      "tensor([[9., 7., 8., 5., 1., 5., 3., 0., 0., 0., 8., 0., 0.]]) tensor([9, 7, 8, 5, 3, 5, 4, 0, 0, 9, 8, 0, 0])\n",
      "tensor([[9., 7., 8., 5., 9., 5., 1., 6., 0., 0., 0., 4., 8.]]) tensor([9, 7, 8, 5, 9, 5, 7, 9, 0, 0, 9, 4, 8])\n",
      "tensor([[9., 7., 8., 5., 9., 4., 3., 8., 4., 2., 2., 3., 7.]]) tensor([9, 7, 8, 5, 9, 4, 7, 7, 4, 2, 2, 3, 7])\n",
      "tensor([[9., 7., 8., 5., 9., 7., 1., 5., 8., 5., 6., 2., 6.]]) tensor([9, 7, 8, 5, 9, 4, 1, 5, 7, 5, 6, 2, 6])\n",
      "tensor([[9., 8., 7., 0., 1., 0., 6., 6., 9., 2., 1., 5., 7.]]) tensor([9, 7, 8, 0, 1, 9, 4, 7, 0, 2, 2, 5, 6])\n",
      "tensor([[8., 7., 1., 0., 1., 0., 0., 3., 4., 6., 7., 8., 0.]]) tensor([9, 7, 8, 0, 1, 9, 4, 3, 5, 7, 3, 4, 0])\n",
      "tensor([[4., 6., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]]) tensor([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 2])\n",
      "tensor([[4., 6., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]]) tensor([4, 6, 0, 1, 4, 5, 0, 0, 3, 0, 0, 1, 0])\n",
      "tensor([[4., 6., 1., 0., 0., 0., 6., 5., 2., 1., 6., 3., 2.]]) tensor([4, 6, 1, 0, 0, 0, 8, 5, 2, 1, 6, 7, 2])\n",
      "tensor([[4., 6., 9., 0., 5., 9., 0., 5., 3., 2., 7., 9., 9.]]) tensor([9, 7, 8, 3, 5, 9, 6, 5, 1, 2, 7, 9, 9])\n",
      "tensor([[9., 7., 8., 5., 9., 5., 3., 2., 0., 0., 6., 5., 2.]]) tensor([9, 7, 8, 5, 9, 5, 4, 2, 0, 0, 6, 5, 2])\n",
      "tensor([[4., 9., 8., 5., 6., 9., 4., 0., 3., 3., 8., 2., 9.]]) tensor([9, 7, 8, 5, 6, 9, 9, 5, 3, 1, 8, 9, 9])\n",
      "tensor([[9., 7., 8., 5., 3., 1., 4., 6., 0., 7., 8., 9., 4.]]) tensor([9, 7, 8, 5, 8, 1, 3, 8, 0, 7, 8, 2, 4])\n",
      "tensor([[4., 6., 0., 0., 0., 0., 6., 0., 0., 0., 6., 5., 4.]]) tensor([4, 6, 0, 0, 6, 0, 5, 0, 1, 7, 3, 3, 3])\n",
      "tensor([[4., 6., 0., 0., 0., 0., 0., 0., 0., 0., 7., 4., 5.]]) tensor([4, 6, 8, 0, 0, 0, 1, 8, 2, 0, 0, 1, 0])\n",
      "tensor([[2., 1., 0., 0., 1., 0., 0., 0., 3., 4., 5., 5., 9.]]) tensor([2, 1, 0, 0, 1, 0, 0, 0, 3, 4, 5, 5, 9])\n",
      "tensor([[5., 4., 1., 0., 3., 2., 8., 2., 0., 1., 2., 3., 4.]]) tensor([5, 5, 5, 0, 1, 1, 8, 9, 0, 1, 2, 3, 2])\n",
      "tensor([[4., 6., 0., 7., 1., 4., 9., 8., 3., 1., 3., 1., 6.]]) tensor([4, 6, 0, 7, 1, 2, 9, 6, 3, 5, 3, 5, 7])\n",
      "tensor([[4., 6., 0., 7., 0., 4., 1., 3., 1., 3., 2., 7., 6.]]) tensor([4, 6, 0, 7, 9, 3, 1, 8, 8, 3, 2, 6, 7])\n",
      "tensor([[4., 6., 0., 3., 3., 7., 7., 5., 7., 3., 7., 8., 8.]]) tensor([4, 6, 0, 3, 3, 2, 2, 5, 6, 7, 7, 7, 8])\n",
      "tensor([[4., 6., 0., 7., 0., 2., 6., 0., 1., 0., 6., 6., 7.]]) tensor([4, 6, 0, 3, 2, 2, 3, 0, 6, 9, 7, 7, 7])\n",
      "tensor([[4., 6., 0., 3., 1., 8., 1., 1., 0., 6., 0., 2., 8.]]) tensor([4, 6, 0, 3, 5, 3, 1, 1, 0, 3, 6, 6, 8])\n",
      "tensor([[4., 6., 0., 7., 0., 3., 8., 7., 7., 0., 0., 5., 9.]]) tensor([4, 6, 0, 2, 7, 3, 1, 7, 4, 1, 1, 2, 7])\n",
      "tensor([[2., 0., 0., 0., 0., 2., 0., 0., 0., 8., 4., 1., 1.]]) tensor([2, 4, 0, 0, 0, 0, 0, 0, 0, 6, 4, 1, 1])\n",
      "tensor([[4., 6., 0., 0., 8., 1., 7., 0., 0., 0., 4., 1., 2.]]) tensor([4, 8, 1, 0, 6, 1, 3, 0, 0, 0, 4, 1, 2])\n",
      "tensor([[4., 6., 1., 3., 0., 1., 4., 0., 0., 1., 0., 1., 4.]]) tensor([8, 6, 1, 3, 0, 1, 1, 0, 0, 1, 0, 1, 4])\n",
      "tensor([[4., 8., 8., 0., 0., 1., 6., 7., 5., 7., 3., 8., 4.]]) tensor([3, 8, 0, 0, 0, 1, 0, 6, 4, 3, 7, 6, 4])\n",
      "tensor([[4., 6., 5., 0., 0., 0., 6., 3., 6., 0., 1., 6., 1.]]) tensor([4, 6, 5, 0, 0, 6, 7, 7, 6, 0, 1, 3, 1])\n",
      "tensor([[4., 6., 0., 7., 0., 9., 3., 7., 7., 0., 6., 3., 5.]]) tensor([4, 6, 0, 7, 0, 0, 7, 3, 6, 1, 3, 8, 5])\n",
      "tensor([[4., 6., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]]) tensor([6, 9, 3, 9, 5, 4, 0, 5, 8, 8, 8, 8, 1])\n",
      "tensor([[4., 6., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]]) tensor([8, 0, 0, 9, 7, 4, 0, 8, 7, 9, 7, 3, 2])\n",
      "tensor([[4., 6., 9., 0., 6., 5., 1., 0., 3., 6., 5., 5., 0.]]) tensor([4, 6, 0, 0, 8, 5, 1, 0, 1, 1, 5, 5, 0])\n",
      "tensor([[5., 8., 1., 2., 3., 0., 2., 0., 0., 0., 0., 5., 5.]]) tensor([4, 8, 1, 2, 3, 0, 2, 0, 0, 0, 0, 5, 5])\n",
      "tensor([[4., 6., 0., 7., 0., 1., 4., 9., 1., 0., 0., 0., 2.]]) tensor([4, 6, 0, 7, 1, 1, 4, 9, 1, 1, 9, 9, 2])\n",
      "tensor([[4., 6., 0., 7., 0., 6., 7., 7., 1., 0., 8., 6., 4.]]) tensor([4, 6, 0, 7, 1, 8, 3, 8, 1, 9, 7, 6, 2])\n",
      "tensor([[4., 6., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]]) tensor([4, 6, 3, 0, 0, 1, 4, 8, 5, 1, 5, 8, 7])\n",
      "tensor([[4., 6., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 3.]]) tensor([4, 6, 0, 7, 1, 1, 9, 7, 0, 1, 8, 3, 3])\n",
      "tensor([[8., 8., 1., 1., 8., 1., 1., 5., 5., 6., 0., 0., 7.]]) tensor([4, 6, 0, 7, 1, 1, 9, 7, 0, 0, 4, 4, 7])\n",
      "tensor([[4., 6., 6., 0., 0., 0., 1., 3., 3., 2., 1., 9., 8.]]) tensor([4, 6, 8, 0, 0, 2, 1, 8, 8, 2, 2, 9, 6])\n",
      "tensor([[4., 6., 1., 0., 0., 0., 6., 8., 3., 3., 5., 2., 4.]]) tensor([9, 7, 8, 5, 9, 0, 6, 8, 3, 7, 4, 2, 4])\n",
      "tensor([[8., 7., 1., 0., 9., 0., 3., 4., 6., 5., 5., 0., 1.]]) tensor([7, 8, 9, 0, 0, 0, 0, 0, 0, 8, 3, 9, 6])\n",
      "tensor([[6., 9., 4., 1., 0., 5., 7., 4., 0., 4., 8., 5., 1.]]) tensor([6, 9, 4, 1, 0, 5, 7, 4, 0, 4, 8, 5, 1])\n",
      "tensor([[4., 6., 1., 0., 0., 0., 3., 3., 9., 0., 8., 8., 6.]]) tensor([4, 6, 1, 0, 0, 2, 8, 8, 9, 0, 1, 3, 0])\n",
      "tensor([[4., 6., 0., 1., 2., 4., 8., 0., 2., 9., 0., 9., 6.]]) tensor([4, 6, 0, 1, 2, 4, 8, 0, 2, 0, 1, 9, 3])\n",
      "tensor([[4., 6., 0., 1., 2., 4., 8., 0., 1., 8., 6., 1., 7.]]) tensor([4, 6, 0, 1, 2, 4, 8, 0, 1, 6, 3, 1, 8])\n",
      "tensor([[4., 6., 0., 1., 2., 4., 8., 0., 1., 5., 5., 8., 6.]]) tensor([4, 6, 0, 1, 2, 4, 8, 0, 1, 5, 5, 3, 3])\n",
      "tensor([[4., 6., 0., 7., 0., 7., 6., 7., 1., 0., 6., 4., 0.]]) tensor([4, 6, 0, 7, 0, 4, 1, 5, 6, 0, 0, 2, 7])\n",
      "tensor([[4., 6., 0., 1., 4., 4., 3., 0., 1., 7., 0., 9., 4.]]) tensor([4, 6, 0, 1, 2, 4, 8, 0, 1, 6, 0, 0, 4])\n",
      "tensor([[4., 6., 0., 7., 0., 1., 8., 2., 1., 1., 0., 6., 9.]]) tensor([4, 6, 0, 7, 0, 4, 3, 2, 0, 0, 9, 6, 9])\n",
      "tensor([[4., 6., 8., 0., 0., 0., 1., 8., 4., 0., 3., 5., 1.]]) tensor([4, 6, 7, 0, 0, 0, 1, 8, 4, 0, 6, 5, 1])\n",
      "tensor([[4., 6., 0., 0., 9., 1., 9., 8., 1., 8., 1., 7., 3.]]) tensor([4, 6, 0, 0, 9, 1, 9, 8, 1, 3, 1, 3, 3])\n",
      "tensor([[2., 0., 0., 0., 0., 0., 0., 4., 4., 4., 4., 5., 0.]]) tensor([2, 0, 4, 0, 0, 0, 0, 4, 4, 4, 4, 4, 0])\n",
      "tensor([[2., 0., 6., 0., 0., 0., 0., 5., 4., 0., 4., 6., 1.]]) tensor([2, 0, 4, 0, 0, 0, 0, 4, 4, 9, 4, 3, 8])\n",
      "tensor([[4., 6., 5., 1., 0., 5., 1., 3., 1., 1., 1., 1., 7.]]) tensor([4, 6, 5, 1, 1, 1, 1, 6, 1, 1, 1, 3, 3])\n",
      "tensor([[4., 6., 0., 6., 0., 0., 8., 7., 5., 7., 7., 0., 4.]]) tensor([4, 6, 0, 6, 0, 0, 8, 3, 5, 3, 8, 9, 4])\n",
      "tensor([[4., 6., 0., 6., 1., 6., 9., 0., 0., 8., 0., 8., 8.]]) tensor([4, 6, 0, 6, 1, 8, 0, 0, 0, 3, 6, 8, 6])\n",
      "tensor([[4., 6., 0., 7., 0., 1., 9., 4., 7., 0., 1., 0., 7.]]) tensor([4, 6, 0, 7, 0, 5, 0, 4, 6, 0, 8, 1, 3])\n",
      "tensor([[5., 9., 0., 7., 2., 3., 4., 1., 0., 0., 5., 7., 3.]]) tensor([4, 6, 0, 0, 9, 9, 9, 0, 2, 4, 5, 7, 3])\n",
      "tensor([[4., 6., 0., 7., 0., 3., 5., 2., 9., 9., 5., 6., 9.]]) tensor([4, 6, 0, 7, 0, 3, 5, 8, 9, 0, 5, 3, 6])\n",
      "tensor([[4., 6., 2., 7., 0., 6., 9., 7., 2., 2., 6., 1., 6.]]) tensor([4, 6, 2, 7, 0, 8, 9, 7, 4, 0, 6, 1, 8])\n",
      "tensor([[4., 6., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]]) tensor([4, 6, 0, 0, 6, 0, 5, 0, 1, 7, 6, 5, 4])\n",
      "tensor([[9., 7., 8., 5., 9., 7., 0., 8., 0., 3., 0., 3., 9.]]) tensor([9, 7, 8, 5, 9, 6, 9, 3, 0, 3, 0, 8, 9])\n",
      "tensor([[5., 9., 0., 0., 5., 1., 7., 3., 2., 0., 3., 2., 4.]]) tensor([5, 9, 0, 0, 5, 1, 6, 3, 2, 0, 3, 2, 4])\n",
      "tensor([[8., 8., 1., 2., 1., 0., 0., 9., 2., 6., 8., 8., 6.]]) tensor([7, 6, 2, 2, 1, 0, 0, 9, 2, 6, 8, 7, 0])\n",
      "tensor([[4., 6., 1., 0., 0., 1., 5., 4., 4., 0., 3., 9., 4.]]) tensor([4, 6, 0, 7, 0, 3, 1, 2, 4, 0, 6, 9, 4])\n",
      "tensor([[4., 6., 0., 0., 6., 1., 6., 5., 4., 1., 8., 1., 8.]]) tensor([4, 6, 8, 0, 0, 3, 6, 9, 1, 0, 7, 8, 6])\n",
      "tensor([[9., 7., 8., 5., 3., 9., 9., 0., 0., 6., 4., 9., 1.]]) tensor([9, 7, 8, 5, 3, 9, 6, 0, 0, 3, 4, 9, 1])\n",
      "tensor([[9., 7., 8., 5., 3., 0., 9., 0., 0., 4., 5., 7., 8.]]) tensor([9, 7, 8, 5, 3, 9, 6, 0, 0, 4, 5, 7, 3])\n",
      "tensor([[9., 7., 8., 5., 3., 7., 9., 5., 1., 7., 4., 8., 1.]]) tensor([9, 7, 8, 5, 8, 1, 2, 5, 1, 8, 4, 3, 1])\n",
      "tensor([[3., 5., 7., 4., 6., 6., 1., 3., 6., 5., 2., 6., 6.]]) tensor([3, 5, 7, 4, 6, 6, 1, 3, 6, 5, 2, 3, 7])\n",
      "tensor([[4., 6., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]]) tensor([4, 6, 0, 0, 9, 4, 9, 1, 2, 3, 1, 3, 4])\n",
      "tensor([[4., 6., 2., 0., 0., 0., 4., 0., 5., 7., 5., 8., 5.]]) tensor([4, 6, 2, 0, 0, 0, 4, 9, 5, 3, 5, 7, 5])\n",
      "tensor([[3., 6., 0., 0., 5., 2., 2., 6., 8., 5., 1., 4., 3.]]) tensor([3, 6, 0, 0, 5, 2, 2, 3, 6, 5, 6, 4, 8])\n",
      "tensor([[4., 6., 0., 7., 0., 1., 5., 5., 9., 5., 6., 3., 5.]]) tensor([9, 7, 8, 5, 9, 3, 0, 1, 0, 0, 2, 6, 6])\n",
      "tensor([[9., 7., 8., 5., 1., 7., 0., 0., 1., 4., 2., 2., 3.]]) tensor([9, 7, 8, 5, 1, 7, 0, 9, 1, 4, 2, 2, 7])\n",
      "tensor([[9., 7., 8., 5., 9., 1., 7., 5., 0., 5., 7., 6., 8.]]) tensor([9, 7, 8, 5, 9, 1, 7, 5, 9, 5, 3, 6, 8])\n",
      "tensor([[5., 0., 5., 4., 1., 3., 1., 0., 5., 0., 1., 7., 1.]]) tensor([5, 0, 5, 4, 1, 3, 1, 0, 5, 0, 3, 6, 1])\n",
      "tensor([[5., 0., 5., 4., 1., 3., 1., 0., 5., 2., 1., 1., 2.]]) tensor([5, 0, 5, 4, 1, 3, 1, 0, 5, 2, 1, 1, 2])\n",
      "tensor([[5., 0., 5., 4., 1., 3., 1., 0., 5., 0., 1., 7., 7.]]) tensor([5, 0, 5, 4, 1, 3, 1, 0, 4, 0, 1, 8, 8])\n",
      "tensor([[5., 0., 1., 0., 9., 9., 3., 3., 6., 3., 9., 8., 0.]]) tensor([5, 0, 1, 0, 9, 9, 3, 3, 8, 8, 0, 8, 0])\n",
      "tensor([[5., 0., 1., 0., 9., 9., 3., 4., 5., 5., 2., 9., 2.]]) tensor([5, 0, 1, 0, 9, 9, 3, 4, 5, 5, 2, 9, 4])\n",
      "tensor([[4., 8., 0., 7., 0., 5., 6., 1., 4., 3., 3., 6., 4.]]) tensor([4, 8, 9, 7, 0, 5, 6, 8, 4, 3, 6, 8, 4])\n",
      "tensor([[4., 6., 0., 2., 3., 0., 1., 2., 0., 1., 4., 7., 3.]]) tensor([4, 6, 0, 2, 7, 0, 1, 2, 0, 1, 4, 8, 7])\n",
      "tensor([[4., 6., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]]) tensor([4, 6, 0, 7, 1, 7, 1, 7, 8, 0, 3, 7, 1])\n",
      "tensor([[5., 9., 0., 5., 3., 9., 7., 2., 7., 5., 1., 4., 3.]]) tensor([5, 9, 9, 5, 3, 2, 7, 2, 7, 5, 1, 4, 7])\n",
      "tensor([[9., 7., 8., 5., 9., 8., 0., 3., 3., 6., 7., 8., 2.]]) tensor([9, 7, 8, 5, 1, 7, 0, 7, 3, 6, 3, 6, 2])\n",
      "tensor([[4., 6., 0., 1., 3., 1., 3., 0., 0., 2., 7., 1., 2.]]) tensor([4, 6, 0, 1, 7, 1, 3, 0, 0, 1, 7, 0, 2])\n",
      "tensor([[8., 6., 2., 3., 7., 9., 9., 5., 2., 0., 0., 0., 1.]]) tensor([4, 6, 0, 6, 6, 3, 1, 3, 4, 0, 0, 0, 1])\n",
      "tensor([[4., 0., 0., 5., 0., 2., 8., 2., 0., 0., 2., 1., 5.]]) tensor([9, 0, 0, 1, 4, 1, 4, 2, 0, 4, 0, 4, 7])\n",
      "tensor([[4., 7., 1., 0., 1., 0., 5., 0., 0., 0., 0., 1., 6.]]) tensor([4, 8, 1, 0, 0, 2, 3, 0, 0, 0, 1, 5, 6])\n",
      "tensor([[4., 6., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]]) tensor([4, 6, 1, 0, 0, 1, 2, 0, 4, 4, 8, 7, 7])\n",
      "tensor([[4., 7., 2., 0., 0., 1., 8., 1., 3., 5., 3., 0., 6.]]) tensor([4, 7, 4, 0, 0, 1, 8, 1, 3, 5, 3, 0, 6])\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "\n",
    "for x, y in test_dataset:\n",
    "    pred = trained_model(x[None, ...])\n",
    "    print(pred, y)\n",
    "    res.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5. 9. 0. 1. 0. 2. 6. 7. 0. 6. 6. 1. 6.]]\n",
      "[[8. 2. 0. 0. 0. 2. 5. 3. 0. 1. 0. 2. 7.]]\n",
      "[[4. 6. 0. 7. 0. 1. 8. 2. 4. 5. 6. 3. 7.]]\n",
      "[[4. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[9. 7. 8. 5. 9. 4. 7. 2. 3. 1. 2. 4. 0.]]\n",
      "[[9. 7. 8. 5. 1. 8. 8. 3. 0. 5. 3. 5. 8.]]\n",
      "[[9. 7. 8. 5. 9. 2. 1. 3. 4. 6. 3. 6. 6.]]\n",
      "[[9. 7. 8. 5. 9. 0. 0. 4. 8. 6. 6. 1. 6.]]\n",
      "[[9. 7. 8. 5. 1. 5. 3. 0. 0. 0. 8. 0. 0.]]\n",
      "[[9. 7. 8. 5. 9. 5. 1. 6. 0. 0. 0. 4. 8.]]\n",
      "[[9. 7. 8. 5. 9. 4. 3. 8. 4. 2. 2. 3. 7.]]\n",
      "[[9. 7. 8. 5. 9. 7. 1. 5. 8. 5. 6. 2. 6.]]\n",
      "[[9. 8. 7. 0. 1. 0. 6. 6. 9. 2. 1. 5. 7.]]\n",
      "[[8. 7. 1. 0. 1. 0. 0. 3. 4. 6. 7. 8. 0.]]\n",
      "[[4. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[4. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[4. 6. 1. 0. 0. 0. 6. 5. 2. 1. 6. 3. 2.]]\n",
      "[[4. 6. 9. 0. 5. 9. 0. 5. 3. 2. 7. 9. 9.]]\n",
      "[[9. 7. 8. 5. 9. 5. 3. 2. 0. 0. 6. 5. 2.]]\n",
      "[[4. 9. 8. 5. 6. 9. 4. 0. 3. 3. 8. 2. 9.]]\n",
      "[[9. 7. 8. 5. 3. 1. 4. 6. 0. 7. 8. 9. 4.]]\n",
      "[[4. 6. 0. 0. 0. 0. 6. 0. 0. 0. 6. 5. 4.]]\n",
      "[[4. 6. 0. 0. 0. 0. 0. 0. 0. 0. 7. 4. 5.]]\n",
      "[[2. 1. 0. 0. 1. 0. 0. 0. 3. 4. 5. 5. 9.]]\n",
      "[[5. 4. 1. 0. 3. 2. 8. 2. 0. 1. 2. 3. 4.]]\n",
      "[[4. 6. 0. 7. 1. 4. 9. 8. 3. 1. 3. 1. 6.]]\n",
      "[[4. 6. 0. 7. 0. 4. 1. 3. 1. 3. 2. 7. 6.]]\n",
      "[[4. 6. 0. 3. 3. 7. 7. 5. 7. 3. 7. 8. 8.]]\n",
      "[[4. 6. 0. 7. 0. 2. 6. 0. 1. 0. 6. 6. 7.]]\n",
      "[[4. 6. 0. 3. 1. 8. 1. 1. 0. 6. 0. 2. 8.]]\n",
      "[[4. 6. 0. 7. 0. 3. 8. 7. 7. 0. 0. 5. 9.]]\n",
      "[[2. 0. 0. 0. 0. 2. 0. 0. 0. 8. 4. 1. 1.]]\n",
      "[[4. 6. 0. 0. 8. 1. 7. 0. 0. 0. 4. 1. 2.]]\n",
      "[[4. 6. 1. 3. 0. 1. 4. 0. 0. 1. 0. 1. 4.]]\n",
      "[[4. 8. 8. 0. 0. 1. 6. 7. 5. 7. 3. 8. 4.]]\n",
      "[[4. 6. 5. 0. 0. 0. 6. 3. 6. 0. 1. 6. 1.]]\n",
      "[[4. 6. 0. 7. 0. 9. 3. 7. 7. 0. 6. 3. 5.]]\n",
      "[[4. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[4. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[4. 6. 9. 0. 6. 5. 1. 0. 3. 6. 5. 5. 0.]]\n",
      "[[5. 8. 1. 2. 3. 0. 2. 0. 0. 0. 0. 5. 5.]]\n",
      "[[4. 6. 0. 7. 0. 1. 4. 9. 1. 0. 0. 0. 2.]]\n",
      "[[4. 6. 0. 7. 0. 6. 7. 7. 1. 0. 8. 6. 4.]]\n",
      "[[4. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[4. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 3.]]\n",
      "[[8. 8. 1. 1. 8. 1. 1. 5. 5. 6. 0. 0. 7.]]\n",
      "[[4. 6. 6. 0. 0. 0. 1. 3. 3. 2. 1. 9. 8.]]\n",
      "[[4. 6. 1. 0. 0. 0. 6. 8. 3. 3. 5. 2. 4.]]\n",
      "[[8. 7. 1. 0. 9. 0. 3. 4. 6. 5. 5. 0. 1.]]\n",
      "[[6. 9. 4. 1. 0. 5. 7. 4. 0. 4. 8. 5. 1.]]\n",
      "[[4. 6. 1. 0. 0. 0. 3. 3. 9. 0. 8. 8. 6.]]\n",
      "[[4. 6. 0. 1. 2. 4. 8. 0. 2. 9. 0. 9. 6.]]\n",
      "[[4. 6. 0. 1. 2. 4. 8. 0. 1. 8. 6. 1. 7.]]\n",
      "[[4. 6. 0. 1. 2. 4. 8. 0. 1. 5. 5. 8. 6.]]\n",
      "[[4. 6. 0. 7. 0. 7. 6. 7. 1. 0. 6. 4. 0.]]\n",
      "[[4. 6. 0. 1. 4. 4. 3. 0. 1. 7. 0. 9. 4.]]\n",
      "[[4. 6. 0. 7. 0. 1. 8. 2. 1. 1. 0. 6. 9.]]\n",
      "[[4. 6. 8. 0. 0. 0. 1. 8. 4. 0. 3. 5. 1.]]\n",
      "[[4. 6. 0. 0. 9. 1. 9. 8. 1. 8. 1. 7. 3.]]\n",
      "[[2. 0. 0. 0. 0. 0. 0. 4. 4. 4. 4. 5. 0.]]\n",
      "[[2. 0. 6. 0. 0. 0. 0. 5. 4. 0. 4. 6. 1.]]\n",
      "[[4. 6. 5. 1. 0. 5. 1. 3. 1. 1. 1. 1. 7.]]\n",
      "[[4. 6. 0. 6. 0. 0. 8. 7. 5. 7. 7. 0. 4.]]\n",
      "[[4. 6. 0. 6. 1. 6. 9. 0. 0. 8. 0. 8. 8.]]\n",
      "[[4. 6. 0. 7. 0. 1. 9. 4. 7. 0. 1. 0. 7.]]\n",
      "[[5. 9. 0. 7. 2. 3. 4. 1. 0. 0. 5. 7. 3.]]\n",
      "[[4. 6. 0. 7. 0. 3. 5. 2. 9. 9. 5. 6. 9.]]\n",
      "[[4. 6. 2. 7. 0. 6. 9. 7. 2. 2. 6. 1. 6.]]\n",
      "[[4. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[9. 7. 8. 5. 9. 7. 0. 8. 0. 3. 0. 3. 9.]]\n",
      "[[5. 9. 0. 0. 5. 1. 7. 3. 2. 0. 3. 2. 4.]]\n",
      "[[8. 8. 1. 2. 1. 0. 0. 9. 2. 6. 8. 8. 6.]]\n",
      "[[4. 6. 1. 0. 0. 1. 5. 4. 4. 0. 3. 9. 4.]]\n",
      "[[4. 6. 0. 0. 6. 1. 6. 5. 4. 1. 8. 1. 8.]]\n",
      "[[9. 7. 8. 5. 3. 9. 9. 0. 0. 6. 4. 9. 1.]]\n",
      "[[9. 7. 8. 5. 3. 0. 9. 0. 0. 4. 5. 7. 8.]]\n",
      "[[9. 7. 8. 5. 3. 7. 9. 5. 1. 7. 4. 8. 1.]]\n",
      "[[3. 5. 7. 4. 6. 6. 1. 3. 6. 5. 2. 6. 6.]]\n",
      "[[4. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[4. 6. 2. 0. 0. 0. 4. 0. 5. 7. 5. 8. 5.]]\n",
      "[[3. 6. 0. 0. 5. 2. 2. 6. 8. 5. 1. 4. 3.]]\n",
      "[[4. 6. 0. 7. 0. 1. 5. 5. 9. 5. 6. 3. 5.]]\n",
      "[[9. 7. 8. 5. 1. 7. 0. 0. 1. 4. 2. 2. 3.]]\n",
      "[[9. 7. 8. 5. 9. 1. 7. 5. 0. 5. 7. 6. 8.]]\n",
      "[[5. 0. 5. 4. 1. 3. 1. 0. 5. 0. 1. 7. 1.]]\n",
      "[[5. 0. 5. 4. 1. 3. 1. 0. 5. 2. 1. 1. 2.]]\n",
      "[[5. 0. 5. 4. 1. 3. 1. 0. 5. 0. 1. 7. 7.]]\n",
      "[[5. 0. 1. 0. 9. 9. 3. 3. 6. 3. 9. 8. 0.]]\n",
      "[[5. 0. 1. 0. 9. 9. 3. 4. 5. 5. 2. 9. 2.]]\n",
      "[[4. 8. 0. 7. 0. 5. 6. 1. 4. 3. 3. 6. 4.]]\n",
      "[[4. 6. 0. 2. 3. 0. 1. 2. 0. 1. 4. 7. 3.]]\n",
      "[[4. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[5. 9. 0. 5. 3. 9. 7. 2. 7. 5. 1. 4. 3.]]\n",
      "[[9. 7. 8. 5. 9. 8. 0. 3. 3. 6. 7. 8. 2.]]\n",
      "[[4. 6. 0. 1. 3. 1. 3. 0. 0. 2. 7. 1. 2.]]\n",
      "[[8. 6. 2. 3. 7. 9. 9. 5. 2. 0. 0. 0. 1.]]\n",
      "[[4. 0. 0. 5. 0. 2. 8. 2. 0. 0. 2. 1. 5.]]\n",
      "[[4. 7. 1. 0. 1. 0. 5. 0. 0. 0. 0. 1. 6.]]\n",
      "[[4. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "[[4. 7. 2. 0. 0. 1. 8. 1. 3. 5. 3. 0. 6.]]\n"
     ]
    }
   ],
   "source": [
    "test_markup = pd.read_csv('test_markup.csv', encoding='utf-16', header=None, names=NAMES)\n",
    "\n",
    "for idx in range(len(res)):\n",
    "    item = res[idx].numpy()\n",
    "    print(item)\n",
    "    out = 0\n",
    "    for i, val in enumerate(reversed(item[0])):\n",
    "        out += val * 10 ** i\n",
    "\n",
    "    test_markup.loc[idx, 'code'] = out\n",
    "    \n",
    "test_markup.to_csv('answer.csv', encoding='utf-16', header=None, index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
