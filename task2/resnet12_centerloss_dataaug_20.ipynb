{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_reader import Vocabulary, HWDBDatasetHelper, LMDBReader\n",
    "\n",
    "# your path to data\n",
    "train_path = r'/DATA/ichuviliaeva/ocr_data/train.lmdb'\n",
    "test_path = r'/DATA/ichuviliaeva/ocr_data/test.lmdb'\n",
    "gt_path = './gt.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avashchilko/abbyy9sem/course_cvdl/.venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reader = LMDBReader(train_path)\n",
    "train_reader.open()\n",
    "train_helper = HWDBDatasetHelper(train_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_helper, val_helper = train_helper.train_val_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2578433, 644609)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_helper.size(), val_helper.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f713cd34370>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from task2pack.utils.train import show_train_plots, train_with_trainable_loss\n",
    "from task2pack.utils.data import HWDBDataset \n",
    "\n",
    "from task2pack.models.resnet import ResNet12GrayscaleFeatPytorch\n",
    "from task2pack.models.loss import CenterLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model = ResNet34GrayscaleFeat(train_helper.vocabulary.num_classes())\n",
    "\"\"\"\n",
    "model = ResNet12GrayscaleFeatPytorch(train_helper.vocabulary.num_classes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvashchilkoav\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/avashchilko/abbyy10sem/course_ocr/task2/wandb/run-20230409_171443-dgknepl8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vashchilkoav/ocr%20task%202/runs/dgknepl8' target=\"_blank\">ResNet12GrayscaleFeatPytorch 20 epochs with lr=0.001 no augment</a></strong> to <a href='https://wandb.ai/vashchilkoav/ocr%20task%202' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vashchilkoav/ocr%20task%202' target=\"_blank\">https://wandb.ai/vashchilkoav/ocr%20task%202</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vashchilkoav/ocr%20task%202/runs/dgknepl8' target=\"_blank\">https://wandb.ai/vashchilkoav/ocr%20task%202/runs/dgknepl8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/vashchilkoav/ocr%20task%202/runs/dgknepl8?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f6d2a60ea90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'ResNet12GrayscaleFeatPytorch'\n",
    "\n",
    "train_transfroms = nn.Sequential(\n",
    "    transforms.Resize((128, 128))\n",
    ")\n",
    "\n",
    "val_transfroms = nn.Sequential(\n",
    "    transforms.Resize((128, 128))\n",
    ")\n",
    "\n",
    "train_dataloader_config = {\n",
    "    'batch_size': 512,\n",
    "    'shuffle': True,\n",
    "    'drop_last': True,\n",
    "    'num_workers': 8,\n",
    "}\n",
    "\n",
    "test_dataloader_config = {\n",
    "    'batch_size': 2048,\n",
    "    'shuffle': False,\n",
    "    'num_workers': 8,\n",
    "}\n",
    "\n",
    "training_config = {\n",
    "    'lr': 1e-3,\n",
    "    'epochs': 20,\n",
    "    'milestones': [40, 50, 75],\n",
    "    'gamma': 0.7,\n",
    "    'weight_criterion': 0.3,\n",
    "    'lr_criterion': 0.5,\n",
    "}\n",
    "\n",
    "device = 'cuda:1'\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "centerloss = CenterLoss(num_classes=train_helper.vocabulary.num_classes(), feat_dim=512)\n",
    "\n",
    "wandb.init(\n",
    "    project='ocr task 2',\n",
    "    name='{} {} epochs with lr={} no augment'.format(model_name, training_config['epochs'], training_config['lr']),\n",
    "    config={\n",
    "        'train_dataloader_config': train_dataloader_config,\n",
    "        'test_dataloader_config': test_dataloader_config,\n",
    "        'training_config': training_config,\n",
    "        'train_transforms': train_transfroms,\n",
    "        'val_transforms': val_transfroms,\n",
    "\n",
    "        \"architecture\": model_name,\n",
    "        \"dataset\": \"CASIA Offline Chinese Handwriting\",\n",
    "        \"criterion\": \"Cross Entropy Loss + Centerloss\",\n",
    "        \"optimizer\": \"Adam + SGD(Centerloss)\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HWDBDataset(train_helper, transforms=train_transfroms)\n",
    "val_dataset = HWDBDataset(val_helper, transforms=val_transfroms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avashchilko/abbyy10sem/course_ocr/task2/task2pack/models/loss.py:30: UserWarning: This overload of addmm_ is deprecated:\n",
      "\taddmm_(Number beta, Number alpha, Tensor mat1, Tensor mat2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddmm_(Tensor mat1, Tensor mat2, *, Number beta, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1420.)\n",
      "  distmat.addmm_(1, -2, x, self.centers.t())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial val: [regular_loss: 0.004350887026771366, trainable_loss: 0.076739758697722, accuracy: 5.7399136530827216e-05]\n",
      "Epoch 1:\n",
      "Train loss: [regular: 0.007463973710231441 trainable: 0.15181989397094015]\n",
      "Epoch 2:\n",
      "Train loss: [regular: 0.0007991192896107778 trainable: 0.038123349090206315]\n",
      "Val : [regular_loss: 0.0002540232281633176, trainable_loss: 0.005424670172062339, accuracy: 0.8819858239646049]\n",
      "Epoch 3:\n",
      "Train loss: [regular: 0.0005518375115188757 trainable: 0.013712905670878744]\n",
      "Epoch 4:\n",
      "Train loss: [regular: 0.0004617943626318037 trainable: 0.006215794948965896]\n",
      "Val : [regular_loss: 0.00013317624369362047, trainable_loss: 0.0011135250057519197, accuracy: 0.932014601099271]\n",
      "Epoch 5:\n",
      "Train loss: [regular: 0.0004017283245075096 trainable: 0.0032570890688544407]\n",
      "Epoch 6:\n",
      "Train loss: [regular: 0.0003548520273372514 trainable: 0.0019002064942830832]\n",
      "Val : [regular_loss: 0.00012160782436326708, trainable_loss: 0.000386351981094622, accuracy: 0.937312386268265]\n",
      "Epoch 7:\n",
      "Train loss: [regular: 0.0003162726678274255 trainable: 0.0012171669903855808]\n",
      "Epoch 8:\n",
      "Train loss: [regular: 0.00028431716822554853 trainable: 0.0008459760241485066]\n",
      "Val : [regular_loss: 0.00012583455878145195, trainable_loss: 0.000200713805383216, accuracy: 0.9340964832945243]\n",
      "Epoch 9:\n",
      "Train loss: [regular: 0.00025712268782882367 trainable: 0.0006298311535748899]\n",
      "Epoch 10:\n",
      "Train loss: [regular: 0.0002336439080221445 trainable: 0.0004947644450586753]\n",
      "Val : [regular_loss: 9.908175156888687e-05, trainable_loss: 0.00011877857375525069, accuracy: 0.9473091439927149]\n",
      "Epoch 11:\n",
      "Train loss: [regular: 0.0002130647717506784 trainable: 0.00040464849179478314]\n",
      "Epoch 12:\n",
      "Train loss: [regular: 0.00019498162539032855 trainable: 0.00034101019140058775]\n",
      "Val : [regular_loss: 9.860006711629353e-05, trainable_loss: 8.39336314877861e-05, accuracy: 0.947662846779986]\n",
      "Epoch 13:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_losses, test_losses, train_centerloss, test_centerloss, trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_with_trainable_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainable_criterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenterloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataloader_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwandb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/abbyy10sem/course_ocr/task2/task2pack/utils/train.py:183\u001b[0m, in \u001b[0;36mtrain_with_trainable_loss\u001b[0;34m(model, criterion, trainable_criterion, device, train_dataset, test_dataset, train_dataloader_kwargs, test_dataloader_kwargs, training_kwargs, wandb_instance, eval_every)\u001b[0m\n\u001b[1;32m    180\u001b[0m trainable_loss_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m trainable_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    182\u001b[0m loss \u001b[38;5;241m=\u001b[39m regular_loss \u001b[38;5;241m+\u001b[39m trainable_loss\n\u001b[0;32m--> 183\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m trainable_criterion\u001b[38;5;241m.\u001b[39mparameters():\n",
      "File \u001b[0;32m~/abbyy9sem/course_cvdl/.venv/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/abbyy9sem/course_cvdl/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses, test_losses, train_centerloss, test_centerloss, trained_model = train_with_trainable_loss(\n",
    "    train_dataset=train_dataset,\n",
    "    test_dataset=val_dataset,\n",
    "    model=model, \n",
    "    criterion=criterion,\n",
    "    trainable_criterion=centerloss,\n",
    "    train_dataloader_kwargs=train_dataloader_config,\n",
    "    test_dataloader_kwargs=test_dataloader_config,\n",
    "    training_kwargs=training_config,\n",
    "    device=device,\n",
    "    wandb_instance=wandb,\n",
    "    eval_every=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_train_plots(train_losses, ctest_losses, 'ResNet34Grayscale CrossEntropy')\n",
    "show_train_plots(train_centerloss, test_centerloss, 'ResNet34Grayscale Centerloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 380/380 [02:54<00:00,  2.17it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trained_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m wandb\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39msummary[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test_accuracy\n\u001b[1;32m     34\u001b[0m wandb\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39msummary[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_transforms\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test_transforms\n\u001b[0;32m---> 36\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[43mtrained_model\u001b[49m\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     37\u001b[0m wandb\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     38\u001b[0m wandb\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./pred.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trained_model' is not defined"
     ]
    }
   ],
   "source": [
    "from course_ocr_t2.evaluate import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "test_path = r'/DATA/ichuviliaeva/ocr_data/test.lmdb'\n",
    "pred_path = './pred.txt'\n",
    "\n",
    "test_reader = LMDBReader(test_path)\n",
    "test_reader.open()\n",
    "test_helper = HWDBDatasetHelper(test_reader, prefix='Test')\n",
    "\n",
    "test_transforms = nn.Sequential(\n",
    "    transforms.Resize((128, 128)),\n",
    ")\n",
    "\n",
    "test_dataset = HWDBDataset(test_helper, transforms=test_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2048, shuffle=False, num_workers=8)\n",
    "\n",
    "preds = []\n",
    "trained_model.eval()\n",
    "with torch.no_grad():\n",
    "    for X, _ in tqdm(test_loader):\n",
    "        logits, _ = trained_model(X.to(torch.float32).to(device))\n",
    "        classes = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        preds.extend(classes)\n",
    "    \n",
    "with open(pred_path, 'w') as f_pred:\n",
    "    for idx, pred in enumerate(preds):\n",
    "        name = test_helper.namelist[idx]\n",
    "        cls = train_helper.vocabulary.class_by_index(pred)\n",
    "        print(name, cls, file=f_pred)\n",
    "        \n",
    "test_accuracy = evaluate('./gt.txt', './pred.txt')\n",
    "wandb.run.summary['test_accuracy'] = test_accuracy\n",
    "wandb.run.summary['test_transforms'] = test_transforms\n",
    "\n",
    "torch.save(trained_model.state_dict(), './model.pth')\n",
    "wandb.save('./model.pth')\n",
    "wandb.save('./pred.txt')\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
